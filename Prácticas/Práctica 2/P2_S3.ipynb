{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2PZlNGRQDbb"
      },
      "source": [
        "# AdaBoost y Gradient boosting con árboles en un dataset de detección de spam\n",
        "\n",
        "Árboles de decisión y todos los ensembles propuestos funcionan en la práctica muy bien en datos tabulares como puede ser el problema de reconocimiento de spam en mails.\n",
        "\n",
        "Se propone pues emplear el dataset id=44 de openml de detección de Spam. Son un total de 4601 muestras con 57 características."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVshKIhiP7eq",
        "outputId": "89dcaeae-e1a5-4c3d-c3f8-368ff20c3c34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4601, 57)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## Descarga del dataset Spam\n",
        "X, y = fetch_openml(data_id=44, as_frame=False, cache=True, return_X_y=True)\n",
        "print(X.shape)\n",
        "\n",
        "## Partición train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TEnSUoZlLu0"
      },
      "source": [
        "## AdaBoost\n",
        "\n",
        "Probemos el clasificador de AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZnYD265lPFb",
        "outputId": "b4a3379c-ec5d-4068-94fb-b9b512663b06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precisión: 93.8%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ab = AdaBoostClassifier()\n",
        "\n",
        "acc=ab.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "print(f'Precisión: {acc:.1%}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yHIBSvSlWe-"
      },
      "source": [
        "**Ejercicio** Teniendo en cuenta la documentación [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) prueba mediante grid search a encontrar mejores parámetros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEkiZLVvQZEb"
      },
      "source": [
        "## GradientBoostingClassifier\n",
        "\n",
        "Probemos el clasificador Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg3WfwU0Qcj7",
        "outputId": "faa1c9d7-9fc1-461a-99c9-3d41be462e3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precisión: 95.2%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gb = GradientBoostingClassifier()\n",
        "\n",
        "acc=gb.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "print(f'Precisión: {acc:.1%}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXAzloCaRP8S"
      },
      "source": [
        "Los principales parámetros del GradientBoosting son:\n",
        "* n_estimators (100)\n",
        "* max_depth (3)\n",
        "* max_features\n",
        "\n",
        "**Ejercicio** prueba diferentes valores (lógicos) de estos parámetros mediante GridSearch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR_t1qOVXtTP"
      },
      "source": [
        "### Acelerar la búsqueda de parámetros: RandomizedSearch\n",
        "\n",
        "Un problema recurrente que tenemos en estos experimentos es el coste temporal de encontrar los mejores parámetros mediante GridSearch. Una alternativa que reduce dicho coste temporal es el [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
        "\n",
        "**Ejercicio** Implementa la misma búsqueda de valores que has realizado con GridSearch pero ahora con RandomizedSearch con n_iter=10 para limitar el número de pruebas a 10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvCsOZ0XZpM1"
      },
      "source": [
        "### Mejorar la búsqueda de parámetros: BayesianOptimization\n",
        "\n",
        "Optimización Bayesiana es una técnica avanzada de búsqueda de parámetros. Para poder emplearla hay que instalar la librería scikit-optimize. El uso de dicha técnica es similar a las ya empleadas con la diferencia que el rango de los parámetros hay que definirlos teniendo en cuenta además el tipo de los mismos: real, entero o categórico. Además los valores a probar no se listan explícitamente si no que se especifica un rango dentro del cual el algoritmo escogerá los valores.\n",
        "\n",
        "La ejecución puede resultar más lenta pero se pueden explorar más parámetros, de hecho se especifica un intervalo en lugar de valores concretos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p7KIfXTaZ_6",
        "outputId": "5bd6d67f-8fa5-4835-aedf-f9dc5a26d12a"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHMW_KMhZscb",
        "outputId": "cb6529d7-6176-49e5-8807-789c03f2b507"
      },
      "outputs": [],
      "source": [
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, IntegerP2_S3.ipynb\n",
        "BS = BayesSearchCV(GradientBoostingClassifier(), G, scoring='accuracy', n_iter=10, refit=True, cv=5, verbose=10)\n",
        "\n",
        "acc = BS.fit(X_train, y_train).score(X_test, y_test)\n",
        "print(f'Precisión: {acc:.1%} con {BS.best_params_}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u07JuR4TNhe"
      },
      "source": [
        "## HistGradientBoostingClassifier\n",
        "Un problema del clasificador GradientBoosting implementado en sklearn es su velocidad. Es un clasificador muy lento de entrenar. Por ello sklearn propone otro tipo de algoritmo de GradientBoosting que soporta paralelismo con OMP además de otras mejoras computacionales basadas en la discretización de las componentes mediante un histograma.\n",
        "\n",
        "Este otro algoritmo se denomina **HistGradientBoostingClassifier**. Su tiempo de ejecución es mucho mejor. Además se pueden obtener mejores resultados.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avluPs8TQvgg",
        "outputId": "ff7eaed7-bb87-48f3-fd02-33a961d89088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precisión: 95.5%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "hgb = HistGradientBoostingClassifier()\n",
        "\n",
        "acc=hgb.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "print(f'Precisión: {acc:.1%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEWZx5IoVGz2"
      },
      "source": [
        "**Ejercicio** Analiza en la documentación [HistGradientBoosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html) los parámetros más relevantes y realiza una búsqueda de parámetros para obtener el mejor clasificador. A la hora de establecer los valores de los parámetros sería interesante fijarnos en los valores por defecto que se han empleado en el ejercicio anterior y poder explorar alrededor de dichos valores por defecto para conseguir mejores resultados.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
