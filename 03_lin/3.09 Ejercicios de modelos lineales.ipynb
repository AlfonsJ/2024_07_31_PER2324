{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.01 Clasificadores generativos vs discriminativos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.01.01:** En relación con las ventajas de los clasificadores generativos y discriminativos, indica la afirmación incorrecta:\n",
    "1. Los generativos son, por lo general, más fáciles de ajustar.\n",
    "2. Los discriminativos suelen ofrecer mejor precisión que los generativos.\n",
    "3. Los generativos son más flexibles en preproceso de características.\n",
    "4. Los generativos facilitan el tratamiento de datos perdidos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 3; los discriminativos son más flexibles en preproceso de características."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.02 Clasificadores naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.02.01:** El clasificador naive Bayes Bernoulli asume que las funciones de probabilidad condicionales de las clases cumplen la asunción naive Bayes,\n",
    "$$p(\\boldsymbol{x}\\mid y=c,\\boldsymbol{\\theta})=\\prod_{d=1}^D \\operatorname{Ber}(x_d\\mid\\theta_{dc})$$\n",
    "donde $x_d\\in\\{0,1\\}$ y $\\theta_{dc}$ es la probabilidad de que $x_d=1$ en la clase $c$. Supón que $\\;C=2$, $\\;p(y=1)=p(y=2)=0.5$, $\\;D=2$, y\n",
    "$\\;\\boldsymbol{\\theta}=[\\boldsymbol{\\theta}_1;\\boldsymbol{\\theta}_2]$ con $\\;\\boldsymbol{\\theta}_1=(0.7, 0.3)^t$ y $\\;\\boldsymbol{\\theta}_2=(0.2, 0.8)^t$. Determina la probabilidad a posteriori de que la muestra $\\;\\boldsymbol{x}=(1,1)^t$ pertenezca a la clase $1$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:**\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\boldsymbol{x}\\mid y=1;\\boldsymbol{\\theta})%\n",
    "&=\\operatorname{Ber}(x_1\\mid 0.7)\\,\\operatorname{Ber}(x_2\\mid 0.3)=0.7\\cdot 0.2=0.14\\\\%\n",
    "p(\\boldsymbol{x}\\mid y=2;\\boldsymbol{\\theta})%\n",
    "&=\\operatorname{Ber}(x_1\\mid 0.2)\\,\\operatorname{Ber}(x_2\\mid 0.8)=0.2\\cdot 0.8=0.16\\\\%\n",
    "p(y=1\\mid\\boldsymbol{x};\\boldsymbol{\\theta})%\n",
    "&=\\frac{p(\\boldsymbol{x}\\mid y=1;\\boldsymbol{\\theta})}\n",
    "{p(\\boldsymbol{x}\\mid y=1;\\boldsymbol{\\theta})+p(\\boldsymbol{x}\\mid y=2;\\boldsymbol{\\theta})}%\n",
    "&&(\\text{priors equiprobables})\\\\%\n",
    "&=\\frac{0.14}{0.14+0.16}=47\\%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.03 Análisis discriminante Gaussiano"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.03.01:** Indica la afirmación incorrecta (o escoge la última opción si las tres primeras son correctas):\n",
    "1. Análisis discriminante Gaussiano asume que las densidades condicionales de las clases son Gaussianas independientes, lo que resulta en discriminantes (log-posteriors) cuadráticas.\n",
    "2. Análisis discriminante lineal asume que las densidades condicionales de las clases son Gaussianas de matriz de covarianzas común, lo que resulta en discriminantes (log-posteriors) lineales.\n",
    "3. LDA coincide con regresión logística (multinomial) en términos de modelo, pero se diferencia mucho en entrenamiento.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.04 Introducción a regresión logística"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.04.01:** Regresión logística es un modelo de clasificación $p(y\\mid\\boldsymbol{x};\\boldsymbol{\\theta})$, donde $\\boldsymbol{x}\\in\\mathbb{R}^D$ es un vector de $D$ características, $y\\in\\{1,\\dotsc,C\\}$ es la etiqueta de clase y $\\boldsymbol{\\theta}$ es un vector de parámetros. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas):\n",
    "1. Si $C=2$, el modelo es de regresión logística binaria.\n",
    "2. Si $C>2$, el modelo es de regresión logística multinomial.\n",
    "3. Si $C>2$, el modelo es de regresión logística multiclase.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.05 Regresión logística binaria"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.05.01:** Sea un problema de clasificación en dos clases, $y\\in\\{0,1\\}$, y sea $p(y\\mid\\boldsymbol{x};\\boldsymbol{\\theta})=\\operatorname{Ber}(y\\mid\\sigma(\\boldsymbol{w}^t\\boldsymbol{x}+b))$ un modelo de regresión logística binaria con $\\boldsymbol{w}=(3,3)^t$ y $b=0$. Determina la probabilidad de que $\\boldsymbol{x}=(0.5,0.5)^t$ pertenezca a la clase $1$ según el modelo dado."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\quad p(y=1\\mid\\boldsymbol{x};\\boldsymbol{\\theta})%\n",
    "=\\sigma(\\boldsymbol{w}^t\\boldsymbol{x}+b)=\\sigma((3, 3)(0.5, 0.5)^t+0)=\\dfrac{1}{1+e^{-3}}=0.9526$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.05.02:** Sea un problema de clasificación en dos clases, $y\\in\\{0,1\\}$, y sea $p(y\\mid\\boldsymbol{x};\\boldsymbol{\\theta})=\\operatorname{Ber}(y\\mid\\sigma(\\boldsymbol{w}^t\\boldsymbol{x}+b))$ un modelo de regresión logística binaria con $\\boldsymbol{w}=(3,3)^t$ y $b=0$. Determina la logit, $f(\\boldsymbol{x}; \\boldsymbol{\\theta})$, así como la frontera y regiones de decisión que induce (con pérdida 0-1)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\quad$ Logit: $\\quad f(\\boldsymbol{x}; \\boldsymbol{\\theta})=b+\\boldsymbol{w}^t\\boldsymbol{x}=3 x_1+3 x_2$\n",
    "\n",
    "Frontera: $\\quad 3 x_1+3 x_2=0\\to x_2=-x_1$\n",
    "\n",
    "Regiones de decisión:\n",
    "$$\\begin{align*}\n",
    "  \\mathcal{R}_1%\n",
    "  &=\\{\\boldsymbol{x}:p(y=1\\mid\\boldsymbol{x};\\boldsymbol{\\theta})>p(y=0\\mid\\boldsymbol{x};\\boldsymbol{\\theta})\\}\\\\%\n",
    "  &=\\left\\{\\boldsymbol{x}:\\frac{p(y=1\\mid\\boldsymbol{x};\\boldsymbol{\\theta})}{p(y=0\\mid\\boldsymbol{x};\\boldsymbol{\\theta})}>1\\right\\}\\\\%\n",
    "  &=\\{\\boldsymbol{x}:f(\\boldsymbol{x};\\boldsymbol{\\theta})>0\\}\\\\%\n",
    "  &=\\{\\boldsymbol{x}:3 x_1+3 x_2>0\\}\\\\%\n",
    "  &=\\{\\boldsymbol{x}:x_2>-x_1\\}\\\\[3mm]%\n",
    "  \\mathcal{R}_0%\n",
    "  &=\\{\\boldsymbol{x}:x_2<-x_1\\}%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.05.03:** Sea $\\mathcal{D}=\\{(\\boldsymbol{x}_n,y_n): \\boldsymbol{x}_n\\in\\mathbb{R}^D, y_n\\in\\{0,1\\}\\}$, un conjunto de muestras no linealmente separables. Se quiere aprender un modelo de clasificación $p(y\\mid\\boldsymbol{x};\\boldsymbol{\\theta})$ que las clasifique sin error (pérdida 0-1 nula). Indica la respuesta correcta:\n",
    "1. Podemos usar regresión logística binaria convencional, esto es, con $p(y=1\\mid\\boldsymbol{x})=\\operatorname{Ber}(y\\mid\\sigma(\\boldsymbol{w}^t\\boldsymbol{x}+b))$.\n",
    "2. No podemos usar regresión logística binaria convencional, pero sí regresión logística binaria con una función de preproceso, $\\phi(\\boldsymbol{x})$, que linearice las muestras.\n",
    "3. No podemos usar regresión logística binaria, convencional o no, pues necesitamos un modelo de clasificación multi-clase.\n",
    "4. No podemos usar regresión logística binaria, convencional o no, pues es un modelo lineal y necesitamos uno no lineal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 2; la 4 es falsa porque puede haber una función de preproceso que linearice las muestras."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.05.04:** $\\;$ Sea $p(y\\mid\\boldsymbol{x};\\boldsymbol{w})=\\operatorname{Ber}(y\\mid\\sigma(\\boldsymbol{w}^t\\boldsymbol{x}))$, $y\\in\\{0,1\\}$, un modelo de regresión logística binaria en notación compacta (homogénea). Sea el conjunto de datos de entrenamiento $\\mathcal{D}=\\{(\\boldsymbol{x}_n,y_n)\\}$, donde $\\boldsymbol{x}_n\\in\\mathbb{R}^D$ y $y_n\\in\\{0,1\\}$. La neg-log-verosimilitud de $\\boldsymbol{w}$ con respecto a $\\mathcal{D}$ es:\n",
    "$$\\operatorname{NLL}(\\boldsymbol{w})=-\\frac{1}{N}\\sum\\nolimits_n y_n\\log\\mu_n+(1-y_n)\\log(1-\\mu_n)$$\n",
    "donde $\\mu_n=\\sigma(a_n)$ con log-odds $a_n=\\boldsymbol{w}^t\\boldsymbol{x}_n$. Demuestra que el gradiente de la neg-log-verosimilitud es:\n",
    "$$\\nabla_{\\boldsymbol{w}}\\operatorname{NLL}(\\boldsymbol{w})=\\frac{1}{N}\\sum\\nolimits_n(\\mu_n-y_n)\\boldsymbol{x}_n$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:**\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial \\operatorname{NLL}}{\\partial w_d}%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n%\n",
    "y_n\\frac{\\partial}{\\partial w_d}\\log\\mu_n%\n",
    "+(1-y_n)\\frac{\\partial}{\\partial w_d}\\log(1-\\mu_n)\\\\%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n%\n",
    "y_n\\frac{1}{\\mu_n}\\frac{\\partial}{\\partial w_d}\\mu_n%\n",
    "-(1-y_n)\\frac{1}{1-\\mu_n}\\frac{\\partial}{\\partial w_d}\\mu_n\\\\%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n%\n",
    "\\left[\\frac{y_n}{\\mu_n}-\\frac{1-y_n}{1-\\mu_n}\\right]%\n",
    "\\frac{\\partial}{\\partial w_d}\\mu_n\\\\%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n%\n",
    "\\frac{y_n(1-\\mu_n)-(1-y_n)\\mu_n}{\\mu_n(1-\\mu_n)}%\n",
    "\\frac{\\partial}{\\partial a_n}\\sigma(a_n)\\frac{\\partial a_n}{\\partial w_d}\\\\%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n%\n",
    "\\frac{y_n-y_n\\mu_n-\\mu_n+y_n\\mu_n}{\\mu_n(1-\\mu_n)}%\n",
    "\\mu_n(1-\\mu_n)x_{nd}\\\\%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n(y_n-\\mu_n)x_{nd}%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.05.05:** Sea $p(y\\mid\\boldsymbol{x};\\boldsymbol{w})=\\operatorname{Ber}(y\\mid\\sigma(\\boldsymbol{w}^t\\boldsymbol{x}))$, $y\\in\\{0,1\\}$, un modelo de regresión logística binaria en notación compacta (homogénea). Sea el conjunto de datos de entrenamiento $\\;\\mathcal{D}=\\{(\\boldsymbol{x}_n,y_n)\\}$, donde $\\;\\boldsymbol{x}_n\\in\\mathbb{R}^D\\;$ y $\\;y_n\\in\\{0,1\\}$. Sabemos que el gradiente de la neg-log-verosimilitud de $\\boldsymbol{w}$ con respecto a $\\mathcal{D}$ es:\n",
    "$$\\nabla_{\\boldsymbol{w}}\\operatorname{NLL}(\\boldsymbol{w})=\\frac{1}{N}\\sum\\nolimits_n(\\mu_n-y_n)\\boldsymbol{x}_n$$\n",
    "donde $\\mu_n=\\sigma(a_n)$ con log-odds $a_n=\\boldsymbol{w}^t\\boldsymbol{x}_n$. Supón que estamos aplicando descenso por gradiente estocástico con minibatch de talla $1$ para minimizar la $\\operatorname{NLL}$. Más concretamente, nos hallamos en la iteración $i$ con $\\boldsymbol{w}_i=(-1, 0, -1)^t$, $\\eta_i=0.1$ y el minibatch actual únicamente incluye la muestra $\\boldsymbol{x}_n=(1, 1, 1)^t$, de clase $y_n=1$. Determina $\\boldsymbol{w}_{i+1}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\quad p(y_n=1\\mid\\boldsymbol{x}_n;\\boldsymbol{w}_t)=\\mu_n=\\sigma(\\boldsymbol{w}_t^t\\boldsymbol{x}_n)=\\sigma(-2)=\\dfrac{1}{1+e^2}=0.12$\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{w}_{i+1}%\n",
    "&=\\boldsymbol{w}_i-\\eta_i\\,(\\mu_n-y_n)\\boldsymbol{x}_n\\\\%\n",
    "&=(-1, 0, -1)^t+0.1\\,(1-0.12)(1, 1, 1)^t\\\\%\n",
    "&=(-1, 0, -1)^t+0.09\\,(1, 1, 1)^t\\\\%\n",
    "&=(-0.91, 0.09, -0.91)^t%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.05.06:** Sea el algoritmo Perceptrón (en clasificación binaria) aplicado a una muestra $(\\boldsymbol{x}_n,y_n)$ para actualizar el vector de parámetros actual, $\\boldsymbol{w}_i$ (supón $x_{n0}=1$ y $b=w_{i0}$) con factor de aprendizaje $\\eta_i=1$. Si $y_n=0$ y $\\hat{y}_n=\\mathbb{I}(\\boldsymbol{w}_i^t\\boldsymbol{x}>0)=1$, entonces:\n",
    "1. $\\boldsymbol{w}_{i+1}=\\boldsymbol{w}_i$\n",
    "2. $\\boldsymbol{w}_{i+1}=\\boldsymbol{w}_i+\\boldsymbol{x}_n$\n",
    "3. $\\boldsymbol{w}_{i+1}=\\boldsymbol{w}_i-\\boldsymbol{x}_n$\n",
    "4. Ninguna de las anteriores."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.05.07:** Repite el ejercicio 3.05.05 con el algoritmo Perceptrón en lugar de descenso por gradiente estocástico."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\quad p(y_n=1\\mid\\boldsymbol{x}_n;\\boldsymbol{w}_t)=\\mu_n=H(\\boldsymbol{w}_t^t\\boldsymbol{x}_n)=H(-2)=0\\quad\\to\\quad\\hat{y}_n=0$\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{w}_{t+1}%\n",
    "&=\\boldsymbol{w}_t-\\eta_t\\,(\\hat{y}_n-y_n)\\boldsymbol{x}_n\\\\%\n",
    "&=(-1, 0, -1)^t+0.1\\,(1-0)(1, 1, 1)^t\\\\%\n",
    "&=(-1, 0, -1)^t+0.1\\,(1, 1, 1)^t\\\\%\n",
    "&=(-0.9, 0.1, -0.9)^t%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.06 Regresión logística multinomial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.06.01:** Sea un problema de clasificación en $C=3$ clases, $y\\in\\{1,2,3\\}$, y sea $p(y\\mid\\boldsymbol{x};\\boldsymbol{\\theta})=\\operatorname{Cat}(y\\mid\\mathcal{S}(\\boldsymbol{W}\\boldsymbol{x}+\\boldsymbol{b}))$ un modelo de regresión logística multinomial con $\\boldsymbol{W}=[1, 1; -1, 1; 0, 0]$ y $\\boldsymbol{b}=\\boldsymbol{0}$. Determina la probabilidad de que $\\boldsymbol{x}=[0.5; 0.5]$ pertenezca a la clase $1$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\quad p(y=1\\mid\\boldsymbol{x};\\boldsymbol{\\theta})=\\mathcal{S}([1, 1; -1, 1; 0, 0][0.5; 0.5])_1%\n",
    "=\\mathcal{S}([1; 0; 0])_1=\\dfrac{e}{e+2}=\\dfrac{1}{1+2/e}=0.5761$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.06.02:** $\\quad$ Sea $\\mu_c=\\mathcal{S}(\\boldsymbol{\\eta})_c$ con logits $\\boldsymbol{\\eta}\\in\\mathbb{R}^C$. Prueba que\n",
    "$\\;\\dfrac{\\partial\\mu_c}{\\partial\\eta_j}=\\mu_c(\\delta_{cj}-\\mu_j)\\;$ donde $\\;\\delta_{cj}=\\mathbb{I}(c=j)$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\quad$ Sea $S=\\sum_ie^{\\eta_i}$ el denominador de la softmax.\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial\\mu_c}{\\partial\\eta_j}%\n",
    "&=\\frac{\\partial}{\\partial\\eta_j}\\,\\left[e^{\\eta_c}S^{-1}\\right]\\\\[2mm]%\n",
    "&=\\left[\\frac{\\partial}{\\partial\\eta_j}\\,e^{\\eta_c}\\right]S^{-1}%\n",
    "+e^{\\eta_c}(-S^{-2})\\frac{\\partial}{\\partial\\eta_j}S\\\\[2mm]%\n",
    "&=(\\delta_{cj}\\,e^{\\eta_c})S^{-1}-e^{\\eta_c}e^{\\eta_j}S^{-2}\\\\[2mm]%\n",
    "&=\\frac{e^{\\eta_c}}{S}\\left(\\delta_{cj}-\\frac{e^{\\eta_j}}{S}\\right)\\\\[2mm]%\n",
    "&=\\mu_c(\\delta_{cj}-\\mu_j)%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.06.03:** $\\;$ Sea un problema de clasificación en $C$ clases, y sea $p(y\\mid\\boldsymbol{x};\\mathbf{W})=\\operatorname{Cat}(y\\mid \\mathcal{S}(\\mathbf{W}^t\\boldsymbol{x}))$, $y\\in\\{1,2,\\dotsc,C\\}$, $\\mathbf{W}\\in\\mathbb{R}^{D\\times C}$, un modelo de regresión logística multinomial en notación compacta (homogénea). Sea el conjunto de entrenamiento $\\mathcal{D}=\\{(\\boldsymbol{x}_n,\\boldsymbol{y}_n)\\}$, donde $\\boldsymbol{x}_n\\in\\mathbb{R}^D$ y $\\boldsymbol{y}_n\\in\\{0,1\\}^C$, $\\sum_cy_{nc}=1$. \n",
    "La neg-log-verosimilitud de $\\mathbf{W}$ con respecto a $\\mathcal{D}$ es:\n",
    "$$\\operatorname{NLL}(\\mathbf{W})=-\\frac{1}{N}\\sum_n\\sum_cy_{nc}\\log\\mu_{nc}$$\n",
    "donde $\\boldsymbol{\\mu}_n=\\mathcal{S}(\\boldsymbol{a}_n)$ con logits $\\boldsymbol{a}_n=\\mathbf{W}^t\\boldsymbol{x}_n$. Demuestra que el gradiente de la neg-log-verosimilitud es:\n",
    "$$\\mathbf{\\nabla}_{\\operatorname{vec}(\\mathbf{W})}\\operatorname{NLL}(\\mathbf{W})=\\frac{1}{N}\\sum_{n=1}^N\\boldsymbol{x}_n(\\boldsymbol{\\mu}_n-\\boldsymbol{y}_n)^t$$\n",
    "Por comodidad, el gradiente recibe el formato de $\\mathbf{W}$, esto es, $\\mathbb{R}^{D\\times C}$. Si el modelo se define con $\\mathbf{W}$ transpuesta, $\\mathbf{W}\\in\\mathbb{R}^{C\\times D}$, el formato del gradiente también se transpone."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\quad$ Sea $\\boldsymbol{w}_j$ el vector (columna) de pesos asociados con la clase $j$\n",
    "$$\\begin{align*}\n",
    "\\mathbf{\\nabla}_{\\boldsymbol{w}_j}\\operatorname{NLL}(\\mathbf{W})%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n\\sum\\nolimits_c%\n",
    "\\frac{y_{nc}}{\\mu_{nc}}\\frac{\\partial\\mu_{nc}}{\\partial \\boldsymbol{w}_j}\\\\%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n\\sum\\nolimits_c%\n",
    "\\frac{y_{nc}}{\\mu_{nc}}\\frac{\\partial\\mu_{nc}}{\\partial\\eta_{nj}}\n",
    "\\frac{\\partial\\eta_{nj}}{\\partial \\boldsymbol{w}_j}\\\\%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n\\sum\\nolimits_c%\n",
    "\\frac{y_{nc}}{\\mu_{nc}}\\mu_{nc}(\\delta_{cj}-\\mu_{nj})\\boldsymbol{x}_n\\\\%\n",
    "&=\\frac{1}{N}\\sum\\nolimits_n\\sum\\nolimits_c y_{nc}(\\mu_{nj}-\\delta_{cj})\\boldsymbol{x}_n\\\\%\n",
    "&=\\frac{1}{N}\\sum\\nolimits_n\\left(%\n",
    "\\left(\\sum\\nolimits_cy_{nc}\\right)\\mu_{nj}\\boldsymbol{x}_n%\n",
    "-\\sum\\nolimits_c\\delta_{cj}y_{nc}\\boldsymbol{x}_n\\right)\\\\%\n",
    "&=\\frac{1}{N}\\sum\\nolimits_n(\\mu_{nj}-y_{nj})\\boldsymbol{x}_n%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.06.04:** Sea un problema de clasificación en $C=3$ clases, y sea $p(y\\mid\\boldsymbol{x};\\mathbf{W})=\\operatorname{Cat}(y\\mid \\mathcal{S}(\\mathbf{W}^t\\boldsymbol{x}))$, $y\\in\\{1,2,\\dotsc,C\\}$, $\\mathbf{W}\\in\\mathbb{R}^{D\\times C}$, un modelo de regresión logística multinomial en notación compacta (homogénea). Sea el conjunto de entrenamiento $\\mathcal{D}=\\{(\\boldsymbol{x}_n,\\boldsymbol{y}_n)\\}$, donde $\\boldsymbol{x}_n\\in\\mathbb{R}^D$ y $\\boldsymbol{y}_n\\in\\{0,1\\}^C$, $\\sum_cy_{nc}=1$. Sabemos que el gradiente de la neg-log-verosimilitud de $\\mathbf{W}$ con respecto a $\\mathcal{D}$ es:\n",
    "$$\\mathbf{\\nabla}_{\\operatorname{vec}(\\mathbf{W})}\\operatorname{NLL}(\\mathbf{W})=\\frac{1}{N}\\sum_{n=1}^N\\boldsymbol{x}_n(\\boldsymbol{\\mu}_n-\\boldsymbol{y}_n)^t$$\n",
    "donde $\\boldsymbol{\\mu}_n=\\mathcal{S}(\\boldsymbol{a}_n)$ con logits $\\boldsymbol{a}_n=\\mathbf{W}^t\\boldsymbol{x}_n$. Supón que estamos aplicando descenso por gradiente estocástico con minibatch de talla $1$ para minimizar la $\\operatorname{NLL}$. Más concretamente, nos hallamos en la iteración $i$ con $\\mathbf{W}_i=[1, 0, 1; -1, 1, -1; 0, 0, 1]$, $\\eta_i=0.1$ y el minibatch actual únicamente incluye la muestra $\\boldsymbol{x}_n=(1, 1, 1)^t$, de clase $y_n=1$. Determina $\\mathbf{W}_{i+1}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:**\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{a}_n%\n",
    "&=\\mathbf{W}_t^t\\boldsymbol{x}_n%\n",
    "=\\begin{pmatrix}1&-1&0\\\\0&1&0\\\\1&-1&1\\end{pmatrix}\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}%\n",
    "=\\begin{pmatrix}0\\\\1\\\\1\\end{pmatrix}\\\\[3mm]%\n",
    "\\boldsymbol{\\mu}_n%\n",
    "&=S(\\boldsymbol{a}_n)%\n",
    "=\\dfrac{1}{1+2e}\\begin{pmatrix}1\\\\e\\\\e\\end{pmatrix}%\n",
    "=\\begin{pmatrix}0.1554\\\\0.4223\\\\0.4223\\end{pmatrix}\\\\[3mm]%\n",
    "\\mathbf{W}_{t+1}%\n",
    "&=\\mathbf{W}_t-\\eta_t\\,\\boldsymbol{x}_n(\\boldsymbol{\\mu}_n-\\boldsymbol{y}_n)^t\\\\%\n",
    "&=\\begin{pmatrix}1&0&1\\\\-1&1&-1\\\\0&0&1\\end{pmatrix}-0.1\\,\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}(-0.8446, 0.4223, 0.4223)\\\\%\n",
    "&=\\begin{pmatrix}1&0&1\\\\-1&1&-1\\\\0&0&1\\end{pmatrix}-\\begin{pmatrix}-0.0845&-0.0845&-0.0845\\\\0.0422&0.0422&0.0422\\\\0.0422&0.0422&0.0422\\end{pmatrix}\\\\%\n",
    "&=\\begin{pmatrix}1.0845&0.0845&1.0845\\\\-1.0422&0.9578&-1.0422\\\\-0.0422&-0.0422&0.9578\\end{pmatrix}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.07 Introducción a regresión lineal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.07.01:** Regresión lineal predice una variable dependiente o respuesta, $y\\in\\mathbb{R}$, a partir de variables independientes, explicativas o regresoras, $\\boldsymbol{x}\\in\\mathbb{R}^D$. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas):\n",
    "1. Se asume que el valor esperado de $y$ es función lineal de $\\boldsymbol{x}$.\n",
    "2. Suele asumirse que las variables explicativas incluyen una constante $x_0=1$ para representar el modelo de forma compacta.\n",
    "3. En ocasiones, las variables regresoras son funciones no lineales de los datos pero el modelo sigue considerándose lineal.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
