{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.00 Modelos paramétricos y no paramétricos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.00.01:** Una dicotomı́a clásica entre modelos de aprendizaje automático distingue entre modelos paramétricos y no paramétricos. En relación con esta dicotomı́a, indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. Los paramétricos estiman un vector de parámetros de dimensión fija a partir de datos (de aprendizaje) y luego, en inferencia, prescinden de los datos.\n",
    "2. Los no parámetricos mantienen los datos (tras el aprendizaje, en inferencia), por lo que puede decirse que el número efectivo de parámetros crece con el número de datos.\n",
    "3. Los no parámetricos suelen definirse en términos de una medida de (di)similitud o función distancia para comparar datos.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.01 K-vecinos más próximos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.01.01:** Sea un problema de clasificación de datos 2d en tres clases, $c\\in\\{1, 2, 3\\}$. Se tienen los siguientes datos de aprendizaje:\n",
    "$$\\begin{array}{cccc}\n",
    "n & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15\\\\\\hline%\n",
    "x_{n1} &1&1&1 &2&2&2&2 &3 &4&4 &6 &7&7&7 &8\\\\%\n",
    "x_{n2} &2&3&8 &1&3&7&8 &6 &5&6 &2 &1&2&3 &1\\\\\\hline%\n",
    "c_n    &1&1&2 &1&1&2&2 &2 &1&2 &3 &3&3&3 &3%\n",
    "\\end{array}$$\n",
    "Clasifica la muestra de test $\\boldsymbol{x}=(5, 5)^t$ por los $K$-vecinos más próximos en distancia Euclídea para $K=5$. En caso de empate de votos, desempata con NN entre las clases empatadas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** Hallamos las distancias Euclídeas al cuadrado entre $\\boldsymbol{x}$ y cada dato:\n",
    "$$\\begin{array}{cccc}\n",
    "n & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15\\\\\\hline%\n",
    "d(\\boldsymbol{x}_n,\\boldsymbol{x}) &25&20&25 &25&13&13&18 &5 &1&2 &10 &20&13&8&25%\n",
    "\\end{array}$$\n",
    "El conjunto $5$ vecinos más próximos de $\\boldsymbol{x}$ es $N_5(\\boldsymbol{x})=\\{9, 10, 8, 14, 11\\}$. Nótese que este conjunto es único; si hubiera algún dato no incluido en este conjunto a distancia al cuadrado $10$, tendríamos otro posible conjunto $5$ vecinos más próximos de $\\boldsymbol{x}$ y escogeríamos uno de ellos al azar.\n",
    "\n",
    "Los $5$ vecinos votan: $1$ a la clase $1$, $2$ a la $2$, y $2$ a la $3$. Observamos que no existe una única clase más votada; tenemos dos clases empatadas a dos votos, la $2$ y la $3$, por lo que debemos desempatar con el NN entre las clases empatadas. Entre los vecinos de las clases $2$ y $3$, el más cercano es el $10$, que pertenece a la clase $2$. Por tanto, $5$-NN clasifica $\\boldsymbol{x}$ en la clase $2$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.01.02:**  El elevado coste espacial y temporal de KNN limita en gran medida su aplicabilidad a grandes conjuntos de datos. En relación con el mismo, indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. Se han propuesto diversas para reducir el coste espacial eliminando datos que no afectan a las fronteras de decisión.\n",
    "2. Con el fin de reducir el coste temporal, se han propuesto numerosas técnicas de búsqueda eficiente de $K$ vecinos, exacta y aproximada (para $d>10$).\n",
    "3. **k-d tree** y **locality sensitive hashing (LSH)** son dos técnicas populares de búsqueda rápida de vecinos basadas en particionamiento del espacio en regiones y hashing, respectivamente.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.01.03:** Una de las principales bondades de KNN viene dada por la relativa facilidad con la que puede aplicarse en tareas de reconocimiento (de conjunto) abierto. En relación con este tipo de tareas, indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. **Novelty detection:** El sistema detecta que la muestra de test es de una clase no vista antes; por ejemplo una cara desconocida.\n",
    "2. **Incremental learning, online learning, life-long learning o continual learning:** El sistema detecta una nueva clase con éxito, pregunta por el id de la nueva clase y la añade a las existentes.\n",
    "3. **Out-of-distribution detection:** Se detecta que la muestra de test no es de clase conocida ni desconocida, sino que procede de una distribución enteramente distinta; p.e., una foto sin cara.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.01.04:** Por simplicidad y eficiencia, se quiere construir un clasificador con un solo prototipo por clase que pueda usarse en **online, one-shot learning.** Esto es, si el clasificador detecta una nueva clase con éxito, pregunta por la etiqueta de la nueva clase y la añade a las existentes. Además, tras la detección de una nueva clase, la adaptación del clasificador no debe suponer un coste computacional significativo; en particular, se asume que no tenemos datos de entrenamiento previos al correspondiente a la nueva clase detectada. Asimismo, se asume que el número de clases es muy elevado y que todas ellas pueden observarse con probabilidad parecida, por lo que es razonable asumir que las probabilidades a priori de las clases son idénticas. Indica cuál de las siguientes propuestas no resulta apropiada en este contexto (o escoge la última opción si las tres primeras son apropiadas).\n",
    "1. El clasificador NN con un prototipo por clase.\n",
    "2. Regresión logística multinomial.\n",
    "3. El clasificador por el centroide más próximo (LDA asumiendo priors idénticos).\n",
    "4. Todas son apropiadas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 2 no resulta apropiada ya que la adición de una nueva clase requeriría datos previos y un re-entrenamiento costoso. La 1 y la 3 son la misma propuesta en realidad."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.02 Aprendizaje de métricas distancia"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.02.01:** Con datos de alta dimensión o estructurados, se suele aprender una función de embedding $\\boldsymbol{e}=f(\\boldsymbol{x})$ y hallar distancias en el espacio de embedding. Si $f$ es una DNN, esta aproximación se conoce como **deep metric learning (DML).** Indica la respuesta correcta.\n",
    "1. Falso, DML no se refiere a la función $f$, sino a la propia función distancia, implementada con una DNN.\n",
    "2. Falso, DML también requiere que el cálculo de distancias se haga con redes.\n",
    "3. Cierto, DML se limita a $f$; la idea básica es que los datos similares se hallen más cercanos que los datos diferentes.\n",
    "4. Cierto, aunque DML no se se limita a $f$, sino que también requiere que el cálculo de distancias se haga con redes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.02.02:** Sea $\\mathcal{D}=\\{(\\boldsymbol{x}_i,y_i):i=1:N\\}$ un conjunto de datos etiquetados y sea $\\mathcal{S}=\\{(i,j):y_i=y_j\\}$ un conjunto de pares similares derivado. De acuerdo con el propósito de **deep metric learning,** si $(i,j)\\in\\mathcal{S}$ pero $(i,k)\\not\\in\\mathcal{S}$, entonces:\n",
    "1. $\\boldsymbol{x}_i$ y $\\boldsymbol{x}_j$ deben estar cerca en el espacio de embedding, con independencia de la cercanía entre $\\boldsymbol{x}_i$ y $\\boldsymbol{x}_k$.\n",
    "2. $\\boldsymbol{x}_i$ y $\\boldsymbol{x}_j$ deben estar cerca en el espacio de embedding, pero $\\boldsymbol{x}_i$ y $\\boldsymbol{x}_k$ deben estar lejos.\n",
    "3. $\\boldsymbol{x}_i$ y $\\boldsymbol{x}_j$ deben estar lejos en el espacio de embedding, con independencia de la cercanía entre $\\boldsymbol{x}_i$ y $\\boldsymbol{x}_k$.\n",
    "4. $\\boldsymbol{x}_i$ y $\\boldsymbol{x}_j$ deben estar lejos en el espacio de embedding, pero $\\boldsymbol{x}_i$ y $\\boldsymbol{x}_k$ deben estar cerca."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.02.03:** Si disponemos de datos etiquetados de $C$ clases, las **pérdidas de clasificación** se usan para ajustar un modelo de clasificación con coste temporal $O(NC)$ y reutilizar las características ocultas como función de embedding. En relación con esta aproximación, indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. Es usual utilizar la penúltima capa ya que generaliza mejor a nuevas clases que la capa final.\n",
    "2. Aunque es una aproximación sencilla y escalable, solo embebe ejemplos en el lado correcto de una frontera de decisión, sin garantizar que ejemplos similares estén cerca y diferentes lejos.\n",
    "3. Un gran inconveniente es que no vale para datos no etiquetados.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.02.04:** Las **pérdidas de ranking** aproximan ejemplos similares y alejan ejemplos diferentes, por lo general sin requerir etiquetas de clase. En relación con las mismas, indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. La pérdida (contrastiva) de pares optimiza una red para acercar todos los pares de datos de la misma clase y, de manera independiente, otra red (siamesa) para alejar todos los pares de datos de clase distinta con al menos un margen $m>0$.\n",
    "2. La pérdida triplete une redes siamesas mediante puntos \"ancla\" con el fin de acercar datos similares y, simultáneamente, alejar datos distintos con al menos un margen $m>0$.\n",
    "3. La pérdida $N$-pares generaliza la triplete tomando, por cada \"ancla\", un ejemplo positivo (similar) y $N-1$ negativos (distintos).\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4. La opción 3 no la hemos visto en teoría, por lo que no va para examen. No obstante, se intuye fácilmente que refuerza el aprendizaje de la triplete contrastando cada par similar con numerosos pares diferentes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.03 Estimación con kernels densidad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.03.01:** Un kernel densidad es una función $\\mathcal{K}:\\mathbb{R}\\to\\mathbb{R}^{\\geq 0}$ que integra a uno y simétrica. Su amplitud puede controlarse mediante un parámetro ancho de banda (**bandwidth**), $h>0$, como sigue:\n",
    "1. $\\mathcal{K}_h(x)=\\mathcal{K}(x+h)$\n",
    "2. $\\mathcal{K}_h(x)=\\mathcal{K}(x-h)$\n",
    "4. $\\mathcal{K}_h(x)=\\frac{1}{h}\\mathcal{K}\\left(\\frac{x}{h}\\right)$\n",
    "4. $\\mathcal{K}_h(x)=-\\frac{1}{h}\\mathcal{K}\\left(-\\frac{x}{h}\\right)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.03.02:** La **Parzen window** o **kernel density estimator (KDE)** puede verse como una generalización de la mixtura de $N$ Gaussianas \"empírica\" (con una Gaussiana hiperesférica centrada en cada dato) a $N$ kernels de amplitud dada por un ancho de banda $h$. En relación con este estimador, indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. No requiere ajuste, salvo la elección de $h$, y no es necesario escoger el número de centros de clúster.\n",
    "2. Requiere mucha memoria y tiempo de evaluación.\n",
    "3. Cuanto menor sea $h$, más suave será KDE.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 3; cuanto **mayor** sea $h$, más suave será KDE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.03.03:** Sea un problema de clasificación en $C$ clases equiprobables y sea $\\mathcal{D}$ un conjunto de $N$ datos de aprendizaje con $N_c=N/C$ datos de cada clase $c$. Prueba que el clasificador KNN puede derivarse como un clasificador generativo de densidades condicionales de las clases modeladas con **balloon KDE.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** Balloon KDE hace crecer un volumen alrededor de la entrada $\\boldsymbol{x}$ hasta encontrar $K$ datos, $V(\\boldsymbol{x})$:\n",
    "$$p(\\boldsymbol{x}\\mid y=c,\\mathcal{D})=\\dfrac{\\dfrac{N_c(\\boldsymbol{x})}{N_c}}{V(\\boldsymbol{x})}$$\n",
    "donde $N_c(\\boldsymbol{x})$ es el número de datos de la clase $c$ en $V(\\boldsymbol{x})$ y $N_c$ es el total de datos de la clase $c$ (en $\\mathcal{D}$).\n",
    "\n",
    "Así, la posterior de la clase $c$ coincide con la estimación (heurística) de KNN:\n",
    "$$p(y=c\\mid\\boldsymbol{x},\\mathcal{D})%\n",
    "=\\dfrac{\\dfrac{N_c(\\boldsymbol{x})}{N_cV(\\boldsymbol{x})}\\dfrac{N_c}{N}}{\\sum_{c'}\\dfrac{N_{c'}(\\boldsymbol{x})}{N_{c'}V(\\boldsymbol{x})}\\dfrac{N_{c'}}{N}}%\n",
    "=\\dfrac{N_c(\\boldsymbol{x})}{\\sum_{c'}N_{c'}(\\boldsymbol{x})}%\n",
    "=\\dfrac{N_c(\\boldsymbol{x})}{K}%\n",
    "=\\dfrac{1}{K}\\sum_{n\\in N_K(\\boldsymbol{x},\\mathcal{D})}\\mathbb{I}(y_n=c)$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
