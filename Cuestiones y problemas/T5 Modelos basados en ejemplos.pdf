{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 Modelos basados en ejemplos\n",
    "\n",
    "## Problemas\n",
    "\n",
    "1. 3-NN 2d L2\n",
    "2. 5-NN 2d L1\n",
    "\n",
    "## Cuestiones\n",
    "\n",
    "1. Paramétricos vs no paramétricos\n",
    "2. DML\n",
    "3. KDE\n",
    "4. KDE h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3-NN 2d L2:** $\\;$ Sea un problema de clasificación de datos 2d en tres clases, $c\\in\\{0, 1, 2\\}$. Se tienen los siguientes datos de aprendizaje:\n",
    "$$\\begin{array}{ccccccccc}\n",
    "    n      & 1 & 2 & 3 & 4 & 5 & 6\\\\\\hline%\n",
    "    x_{n1} & 2 & 3 & 4 & 5 & 6 & 7\\\\%\n",
    "    x_{n2} & 3 & 0 & 4 & 1 & 5 & 3\\\\\\hline%\n",
    "    c_n    & 1 & 1 & 2 & 0 & 0 & 0%\n",
    "\\end{array}$$\n",
    "\n",
    "Clasifica la muestra $\\boldsymbol{x}=(4, 3)^t$ por los $3$-vecinos más próximos en distancia Euclídea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3-NN 2d L2:** $\\;$ Sea un problema de clasificación de datos 2d en tres clases, $c\\in\\{0, 1, 2\\}$. Se tienen los siguientes datos de aprendizaje:\n",
    "$$\\begin{array}{ccccccccc}\n",
    "    n      & 1 & 2 & 3 & 4 & 5 & 6\\\\\\hline%\n",
    "    x_{n1} & 2 & 3 & 4 & 5 & 6 & 7\\\\%\n",
    "    x_{n2} & 3 & 0 & 4 & 1 & 5 & 3\\\\\\hline%\n",
    "    c_n    & 1 & 1 & 2 & 0 & 0 & 0%\n",
    "\\end{array}$$\n",
    "\n",
    "Clasifica la muestra $\\boldsymbol{x}=(4, 3)^t$ por los $3$-vecinos más próximos en distancia Euclídea.\n",
    "\n",
    "**Solución:**\n",
    "\n",
    "Hallamos las distancias (Euclídeas) entre $\\boldsymbol{x}$ y cada dato:\n",
    "$$\\begin{array}{ccccccc}\n",
    "n                                  &1 &2 &3 &4 &5 &6\\\\\\hline%\n",
    "d(\\boldsymbol{x}_n,\\boldsymbol{x}) &2 &\\sqrt{10} &1 &\\sqrt{5} &\\sqrt{8} &3\n",
    "\\end{array}$$\n",
    "\n",
    "El conjunto de $3$ vecinos más próximos de $\\boldsymbol{x}$ es $N_3(\\boldsymbol{x})=\\{3, 1, 4\\}$. Cada vecino vota a una clase distinta de las tres que tenemos. Como hay empate a tres, decidimos por el vecino más cercano entre los tres, que es el $3$, de la clase $2$.  Por tanto, $3$-NN clasifica $\\boldsymbol{x}$ en la clase $2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3-NN 2d L2:** $\\;$ Sea un problema de clasificación de datos 2d en tres clases, $c\\in\\{0, 1, 2\\}$. Se tienen los siguientes datos de aprendizaje:\n",
    "$$\\begin{array}{ccccccccc}\n",
    "    n      & 1 & 2 & 3 & 4 & 5 & 6\\\\\\hline%\n",
    "    x_{n1} & 2 & 3 & 4 & 5 & 6 & 7\\\\%\n",
    "    x_{n2} & 3 & 0 & 4 & 1 & 5 & 3\\\\\\hline%\n",
    "    c_n    & 1 & 1 & 2 & 0 & 0 & 0%\n",
    "\\end{array}$$\n",
    "\n",
    "Clasifica la muestra $\\boldsymbol{x}=(4, 3)^t$ por los $3$-vecinos más próximos en distancia Euclídea.\n",
    "\n",
    "**Solución a máquina:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = [[3 1 4]]   d = [[1.         2.         2.23606798]]   c = [[2. 1. 0.]]\n",
      "votes for [0. 1. 2.] :  [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np; from sklearn.neighbors import NearestNeighbors\n",
    "Xy = np.array([ [2, 3, 1], [3, 0, 1], [4, 4, 2], [5, 1, 0], [6, 5, 0], [7, 3, 0]], dtype=float)\n",
    "X = Xy[:, :2]; y = Xy[:, 2]; x = np.array([4, 3], dtype=float); K = 3\n",
    "KNN = NearestNeighbors(n_neighbors=K).fit(X)\n",
    "dist, ind = KNN.kneighbors([x]); print('n =', ind + 1, '  d =', dist, '  c =', y[ind])\n",
    "classes, votes = np.unique(y[ind], return_counts=True); print('votes for', classes, ': ', votes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El NN entre las clases empatadas al máximo de votos es de la clase 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5-NN 2d L1:** $\\;$ Sea un problema de clasificación de datos 2d en tres clases, $c\\in\\{0, 1, 2\\}$. Se tienen los siguientes datos de aprendizaje:\n",
    "$$\\begin{array}{ccccccccc}\n",
    "    n     & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\\\\\hline%\n",
    "    x_{n1} & 5 & 6 & 7 & 2 & 2 & 2 & 2 & 4\\\\%\n",
    "    x_{n2} & 1 & 1 & 3 & 0 & 3 & 4 & 5 & 4\\\\\\hline%\n",
    "    c_n   & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 2%\n",
    "\\end{array}$$\n",
    "Clasifica la muestra $\\boldsymbol{x}=(4, 3)^t$ por los $5$-vecinos más próximos en distancia L1 (Manhattan)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5-NN 2d L1:** $\\;$ Sea un problema de clasificación de datos 2d en tres clases, $c\\in\\{0, 1, 2\\}$. Se tienen los siguientes datos de aprendizaje:\n",
    "$$\\begin{array}{ccccccccc}\n",
    "    n     & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\\\\\hline%\n",
    "    x_{n1} & 5 & 6 & 7 & 2 & 2 & 2 & 2 & 4\\\\%\n",
    "    x_{n2} & 1 & 1 & 3 & 0 & 3 & 4 & 5 & 4\\\\\\hline%\n",
    "    c_n   & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 2%\n",
    "\\end{array}$$\n",
    "Clasifica la muestra $\\boldsymbol{x}=(4, 3)^t$ por los $5$-vecinos más próximos en distancia L1 (Manhattan).\n",
    "\n",
    "**Solución:**\n",
    "\n",
    "Hallamos las distancias L1 entre $\\boldsymbol{x}$ y cada dato:\n",
    "$$\\begin{array}{ccccccccc}\n",
    "n                                  & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\\\\\hline%\n",
    "d(\\boldsymbol{x}_n,\\boldsymbol{x}) & 3 & 4 & 3 & 5 & 2 & 3 & 4 & 1%\n",
    "\\end{array}$$\n",
    "\n",
    "El conjunto de $5$ vecinos más próximos de $\\boldsymbol{x}$ es $N_5(\\boldsymbol{x})=\\{8, 5, 1, 3, 6\\}$. Los $5$ vecinos votan: $1$ a la clase $2$, $2$ a la $1$, y $2$ a la $0$. Observamos que no existe una única clase más votada; tenemos dos clases empatadas a dos votos, la $0$ y la $1$, por lo que debemos desempatar con el NN entre las clases empatadas. Entre los vecinos de las clases $0$ y $1$, el más cercano es el $5$, que pertenece a la clase $1$. Por tanto, $5$-NN clasifica $\\boldsymbol{x}$ en la clase $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5-NN 2d L1:** $\\;$ Sea un problema de clasificación de datos 2d en tres clases, $c\\in\\{0, 1, 2\\}$. Se tienen los siguientes datos de aprendizaje:\n",
    "$$\\begin{array}{ccccccccc}\n",
    "    n     & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\\\\\hline%\n",
    "    x_{n1} & 5 & 6 & 7 & 2 & 2 & 2 & 2 & 4\\\\%\n",
    "    x_{n2} & 1 & 1 & 3 & 0 & 3 & 4 & 5 & 4\\\\\\hline%\n",
    "    c_n   & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 2%\n",
    "\\end{array}$$\n",
    "Clasifica la muestra $\\boldsymbol{x}=(4, 3)^t$ por los $5$-vecinos más próximos en distancia L1 (Manhattan).\n",
    "\n",
    "**Solución a máquina:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = [[8 5 3 6 1]]   d = [[1. 2. 3. 3. 3.]]   c = [[2. 1. 0. 1. 0.]]\n",
      "votes for [0. 1. 2.] :  [2 2 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np; from sklearn.neighbors import NearestNeighbors\n",
    "Xy = np.array([[5,1,0],[6,1,0],[7,3,0],[2,0,1],[2,3,1],[2,4,1],[2,5,1],[4,4,2]],dtype=float)\n",
    "X = Xy[:, :2]; y = Xy[:, 2]; x = np.array([4, 3], dtype=float); K = 5\n",
    "KNN = NearestNeighbors(n_neighbors=K, p=1).fit(X)\n",
    "dist, ind = KNN.kneighbors([x]); print('n =', ind + 1, '  d =', dist, '  c =', y[ind])\n",
    "classes, votes = np.unique(y[ind], return_counts=True); print('votes for', classes, ': ', votes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El NN entre las clases empatadas al máximo de votos es de la clase 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paramétricos vs no paramétricos:** $\\;$ Una dicotomía clásica entre modelos de aprendizaje automático distingue entre modelos paramétricos y no paramétricos. En relación con esta dicotomía, indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. Los paramétricos estiman un vector de parámetros de dimensión fija a partir de datos (de aprendizaje) y luego, en inferencia, prescinden de los datos.\n",
    "2. Los no parámetricos mantienen los datos (tras el aprendizaje, en inferencia), por lo que puede decirse que el número efectivo de parámetros crece con el número de datos.\n",
    "3. El clasificador por los $K$ vecinos más próximos es un ejemplo clásico de modelo paramétrico pues se define en términos de una medida de (di)similitud o función distancia cuyos parámetros debemos aprender.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paramétricos vs no paramétricos:** $\\;$ Una dicotomía clásica entre modelos de aprendizaje automático distingue entre modelos paramétricos y no paramétricos. En relación con esta dicotomía, indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. Los paramétricos estiman un vector de parámetros de dimensión fija a partir de datos (de aprendizaje) y luego, en inferencia, prescinden de los datos.\n",
    "2. Los no parámetricos mantienen los datos (tras el aprendizaje, en inferencia), por lo que puede decirse que el número efectivo de parámetros crece con el número de datos.\n",
    "3. El clasificador por los $K$ vecinos más próximos es un ejemplo clásico de modelo paramétrico pues se define en términos de una medida de (di)similitud o función distancia cuyos parámetros debemos aprender.\n",
    "4. Todas son correctas.\n",
    "\n",
    "**Solución:**\n",
    "\n",
    "La 3 es incorrecta. KNN es un ejemplo clásico de modelo no paramétrico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DML:** $\\;$ Sea $\\mathcal{D}=\\{(\\boldsymbol{x}_i,y_i):i=1:N\\}$ un conjunto de datos etiquetados y sea $\\mathcal{S}=\\{(i,j):y_i=y_j\\}$ un conjunto de pares similares derivado. De acuerdo con el propósito de **deep metric learning,** si $(i,j)\\in\\mathcal{S}$ pero $(i,k)\\not\\in\\mathcal{S}$, entonces $\\boldsymbol{x}_i$ y $\\boldsymbol{x}_j$ deben estar:\n",
    "1. Lejos en el espacio de embedding, con independencia de la cercanía entre $\\boldsymbol{x}_i$ y $\\boldsymbol{x}_k$.\n",
    "2. Lejos en el espacio de embedding, pero $\\boldsymbol{x}_i$ y $\\boldsymbol{x}_k$ deben estar cerca.\n",
    "3. Cerca en el espacio de embedding, con independencia de la cercanía entre $\\boldsymbol{x}_i$ y $\\boldsymbol{x}_k$.\n",
    "4. Cerca en el espacio de embedding, pero $\\boldsymbol{x}_i$ y $\\boldsymbol{x}_k$ deben estar lejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DML:** $\\;$ Sea $\\mathcal{D}=\\{(\\boldsymbol{x}_i,y_i):i=1:N\\}$ un conjunto de datos etiquetados y sea $\\mathcal{S}=\\{(i,j):y_i=y_j\\}$ un conjunto de pares similares derivado. De acuerdo con el propósito de **deep metric learning,** si $(i,j)\\in\\mathcal{S}$ pero $(i,k)\\not\\in\\mathcal{S}$, entonces $\\boldsymbol{x}_i$ y $\\boldsymbol{x}_j$ deben estar:\n",
    "1. Lejos en el espacio de embedding, con independencia de la cercanía entre $\\boldsymbol{x}_i$ y $\\boldsymbol{x}_k$.\n",
    "2. Lejos en el espacio de embedding, pero $\\boldsymbol{x}_i$ y $\\boldsymbol{x}_k$ deben estar cerca.\n",
    "3. Cerca en el espacio de embedding, con independencia de la cercanía entre $\\boldsymbol{x}_i$ y $\\boldsymbol{x}_k$.\n",
    "4. Cerca en el espacio de embedding, pero $\\boldsymbol{x}_i$ y $\\boldsymbol{x}_k$ deben estar lejos.\n",
    "\n",
    "**Solución:**\n",
    "\n",
    "La 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KDE:** $\\;$ La **Parzen window** o **kernel density estimator (KDE)** puede verse como una generalización de la mixtura de $N$ Gaussianas \"empírica\" (con una Gaussiana hiperesférica centrada en cada dato) a $N$ kernels de amplitud dada por un ancho de banda $h$. En relación con este estimador, indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. No requiere ajuste, salvo la elección de $h$, y no es necesario escoger el número de centros de clúster.\n",
    "2. Requiere mucha memoria y tiempo de evaluación.\n",
    "3. Cuanto menor sea $h$, más suave será KDE.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KDE:** $\\;$ La **Parzen window** o **kernel density estimator (KDE)** puede verse como una generalización de la mixtura de $N$ Gaussianas \"empírica\" (con una Gaussiana hiperesférica centrada en cada dato) a $N$ kernels de amplitud dada por un ancho de banda $h$. En relación con este estimador, indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. No requiere ajuste, salvo la elección de $h$, y no es necesario escoger el número de centros de clúster.\n",
    "2. Requiere mucha memoria y tiempo de evaluación.\n",
    "3. Cuanto menor sea $h$, más suave será KDE.\n",
    "4. Todas son correctas.\n",
    "\n",
    "**Solución:**\n",
    "\n",
    "La 3 es incorrecta; cuanto mayor sea $h$, más suave será KDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KDE h:** $\\;$ Un kernel densidad es una función $\\mathcal{K}:\\mathbb{R}\\to\\mathbb{R}^{\\geq 0}$ que integra a uno y simétrica. Su amplitud puede controlarse mediante un parámetro ancho de banda (**bandwidth**), $h>0$, como sigue:\n",
    "1. $\\mathcal{K}_h(x)=h\\mathcal{K}\\left(x\\right)$\n",
    "2. $\\mathcal{K}_h(x)=h\\mathcal{K}\\left(\\frac{x}{h}\\right)$\n",
    "3. $\\mathcal{K}_h(x)=\\frac{1}{h}\\mathcal{K}\\left(x\\right)$\n",
    "4. $\\mathcal{K}_h(x)=\\frac{1}{h}\\mathcal{K}\\left(\\frac{x}{h}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KDE h:** $\\;$ Un kernel densidad es una función $\\mathcal{K}:\\mathbb{R}\\to\\mathbb{R}^{\\geq 0}$ que integra a uno y simétrica. Su amplitud puede controlarse mediante un parámetro ancho de banda (**bandwidth**), $h>0$, como sigue:\n",
    "1. $\\mathcal{K}_h(x)=h\\mathcal{K}\\left(x\\right)$\n",
    "2. $\\mathcal{K}_h(x)=h\\mathcal{K}\\left(\\frac{x}{h}\\right)$\n",
    "3. $\\mathcal{K}_h(x)=\\frac{1}{h}\\mathcal{K}\\left(x\\right)$\n",
    "4. $\\mathcal{K}_h(x)=\\frac{1}{h}\\mathcal{K}\\left(\\frac{x}{h}\\right)$\n",
    "\n",
    "**Solución:**\n",
    "\n",
    "La 4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
