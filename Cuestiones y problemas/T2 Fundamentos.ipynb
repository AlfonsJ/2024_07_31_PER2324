{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2 Fundamentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algebra:** $\\;$ Sea $\\mathbf{X}\\in\\mathbb{R}^{N\\times D}$ una matriz de $N$ datos $D$-dimensionales. Su matriz de dispersión es:\n",
    "1. $\\mathbf{S}=\\mathbf{X}^t\\mathbf{X}$\n",
    "2. $\\mathbf{S}=\\mathbf{X}\\mathbf{X}^t$\n",
    "3. $\\mathbf{S}=\\sum_{n=1}^N(\\boldsymbol{x}_n-\\bar{\\boldsymbol{x}})(\\boldsymbol{x}_n-\\bar{\\boldsymbol{x}})^t$\n",
    "4. $\\mathbf{S}=\\frac{1}{N}\\sum_{n=1}^N(\\boldsymbol{x}_n-\\bar{\\boldsymbol{x}})(\\boldsymbol{x}_n-\\bar{\\boldsymbol{x}})^t$\n",
    "\n",
    "<details><summary>Solución:</summary><br>La 3; la 1 es la suma de cuadrados, la 2 es la matriz de Gram y la 4 la de varianzas (dispersión normalizada).</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algebra:** $\\;$ Obtén una descomposición propia de la matriz de varianzas $\\,\\mathbf{\\Sigma}=\\begin{pmatrix}9&0\\\\0&4\\end{pmatrix}$.\n",
    "\n",
    "<details><summary>Solución:</summary><br>\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbf{\\Sigma}&=\\mathbf{U}\\mathbf{\\Lambda}\\mathbf{U}^t%\n",
    "\\quad\\text{con}\\quad%\n",
    "\\mathbf{U}=\\mathbf{I}%\n",
    "\\quad\\text{y}\\quad%\n",
    "\\mathbf{\\Lambda}=\\operatorname{diag}(9, 4)\\\\[3mm]%\n",
    "% \\bW_{\\text{pca}}&=\\mathbf{\\Lambda}^{-1/2}\\mathbf{U}^t%\n",
    "% =\\operatorname{diag}(1/3, 1/2)\n",
    "\\end{align*}$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algebra:** $\\;$ La matriz $\\,\\mathbf{A}=\\operatorname{diag}(-1, -2)\\,$ es:\n",
    "1. Definida positiva.\n",
    "2. Definida negativa.\n",
    "3. Indefinida.\n",
    "4. Ninguna de las anteriores.\n",
    "\n",
    "<details><summary>Solución:</summary><br>La 2 ya que todos sus valores propios son negativos.</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algebra:** $\\;$ Sea $\\,\\mathcal{D}=\\{\\boldsymbol{x}_1,\\dotsc,\\boldsymbol{x}_N\\}\\,$ un conjunto de $N$ datos $D$ dimensionales que queremos reducir a $K$ dimensiones, $\\,K\\ll D,\\,$ mediante PCA; esto es, hallando una proyección lineal ortogonal $\\,\\mathbf{W}\\in\\mathbb{R}^{D\\times K}\\,$ que minimice el error de reconstrucción. Este objetivo puede expresarse formalmente como:\n",
    "1. $\\mathcal{L}(\\mathbf{W})=\\frac{1}{N}\\sum_n\\lVert\\boldsymbol{x}_n-\\operatorname{decode}(\\operatorname{encode}(\\boldsymbol{x}_n))\\rVert_2^2\\quad\\text{con}\\quad\\operatorname{encode}(\\boldsymbol{x})=\\mathbf{W}^t\\boldsymbol{x}\\quad\\text{y}\\quad\\operatorname{decode}(\\boldsymbol{z})=\\mathbf{W}\\boldsymbol{z}$\n",
    "2. $\\mathcal{L}(\\mathbf{W})=\\frac{1}{N}\\sum_n\\lVert\\boldsymbol{x}_n-\\operatorname{decode}(\\operatorname{encode}(\\boldsymbol{x}_n))\\rVert_2^2\\quad\\text{con}\\quad\\operatorname{encode}(\\boldsymbol{x})=\\mathbf{W}\\boldsymbol{x}\\quad\\text{y}\\quad\\operatorname{decode}(\\boldsymbol{z})=\\mathbf{W}^t\\boldsymbol{z}$\n",
    "3. $\\mathcal{L}(\\mathbf{W})=\\frac{1}{N}\\sum_n\\lVert\\boldsymbol{x}_n-\\operatorname{encode}(\\operatorname{decode}(\\boldsymbol{x}_n))\\rVert_2^2\\quad\\text{con}\\quad\\operatorname{encode}(\\boldsymbol{x})=\\mathbf{W}^t\\boldsymbol{x}\\quad\\text{y}\\quad\\operatorname{decode}(\\boldsymbol{z})=\\mathbf{W}\\boldsymbol{z}$\n",
    "4. $\\mathcal{L}(\\mathbf{W})=\\frac{1}{N}\\sum_n\\lVert\\boldsymbol{x}_n-\\operatorname{encode}(\\operatorname{decode}(\\boldsymbol{x}_n))\\rVert_2^2\\quad\\text{con}\\quad\\operatorname{encode}(\\boldsymbol{x})=\\mathbf{W}\\boldsymbol{x}\\quad\\text{y}\\quad\\operatorname{decode}(\\boldsymbol{z})=\\mathbf{W}^t\\boldsymbol{z}$\n",
    "\n",
    "<details><summary>Solución:</summary><br>La 1.</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algebra:** $\\;$ Sea $\\,\\mathcal{D}=\\{\\boldsymbol{x}_1=(-1, -1)^t,\\boldsymbol{x}_2=(1, 3)^t,\\boldsymbol{x}_1=(3, 1)^t,\\boldsymbol{x}_1=(3, 3)^t\\}\\,$ un conjunto de $N=4$ datos de $D=2$ dimensiones que queremos reducir a $K=1$ dimensión. Codifica, decodifica y halla la distorsión de los datos con $\\,\\mathbf{W}=(\\sqrt{2}/2, \\sqrt{2}/2)^t$.\n",
    "\n",
    "<details><summary>Solución:</summary><br>\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{x}_1&=(-1,-1)^t&\\boldsymbol{x}_2&=(1,3)^t&\\boldsymbol{x}_3&=(3,1)^t&\\boldsymbol{x}_4&=(3,3)^t\\\\\n",
    "z_1&=\\mathbf{W}^t\\boldsymbol{x}_1=-\\sqrt{2}&z_2&=2\\sqrt{2}&z_3&=2\\sqrt{2}&z_4&=3\\sqrt{2}\\\\\n",
    "\\hat{\\boldsymbol{x}}_1&=\\mathbf{W}z_1=(-1,-1)^t&\\hat{\\boldsymbol{x}}_2&=(2,2)^t&\\hat{\\boldsymbol{x}}_3&=(2,2)^t&\\hat{\\boldsymbol{x}}_4&=(3,3)^t\n",
    "\\end{align*}$$\n",
    "$$\\mathcal{L}(\\mathbf{W})=\\frac{1}{4}\\left(\\lVert\\boldsymbol{x}_1-\\hat{\\boldsymbol{x}}_1\\rVert_2^2+\\lVert\\boldsymbol{x}_2-\\hat{\\boldsymbol{x}}_2\\rVert_2^2+\\lVert\\boldsymbol{x}_3-\\hat{\\boldsymbol{x}}_3\\rVert_2^2+\\lVert\\boldsymbol{x}_4-\\hat{\\boldsymbol{x}}_4\\rVert_2^2\\right)%\n",
    "=\\frac{1}{4}\\left(0+2+2+0\\right)=1$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algebra:** $\\;$ Sea $\\,\\mathcal{D}=\\{\\boldsymbol{x}_1=(-1, -1)^t,\\boldsymbol{x}_2=(1, 3)^t,\\boldsymbol{x}_1=(3, 1)^t,\\boldsymbol{x}_1=(3, 3)^t\\}\\,$ un conjunto de $N=4$ datos de $D=2$ dimensiones que queremos reducir a $K=1$ dimensión. Halla la proyección lineal ortogonal $\\,\\mathbf{W}\\in\\mathbb{R}^{D\\times K}\\,$ que escoge PCA.\n",
    "\n",
    "<details><summary>Solución:</summary><br>\n",
    "\n",
    "$$\\boldsymbol{\\mu}=\\begin{pmatrix}3/2\\\\3/2\\end{pmatrix},\\;%\n",
    "\\mathbf{\\Sigma}=\\begin{pmatrix}11/4&7/4\\\\7/4&11/4\\end{pmatrix}%\n",
    "=\\mathbf{U}\\mathbf{\\Lambda}\\mathbf{U}^t%\n",
    "\\;\\text{con}\\;%\n",
    "\\mathbf{U}=\\begin{pmatrix}\\frac{\\sqrt{2}}{2}&-\\frac{\\sqrt{2}}{2}\\\\\\frac{\\sqrt{2}}{2}&\\frac{\\sqrt{2}}{2}\\end{pmatrix}%\n",
    "\\;\\text{y}\\;%\n",
    "\\mathbf{\\Lambda}=\\begin{pmatrix}\\frac{9}{2}&0\\\\0&1\\end{pmatrix}%\n",
    "\\quad\\to\\quad\\mathbf{W}=\\begin{pmatrix}\\frac{\\sqrt{2}}{2}\\\\\\frac{\\sqrt{2}}{2}\\end{pmatrix}$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algebra:** $\\;$ Sea $\\mathbf{X}$ una matriz de $N$ datos centrados de $D=2$ dimensiones que queremos reducir a $K=1$ dimensión. Halla la proyección lineal ortogonal $\\,\\mathbf{W}\\in\\mathbb{R}^{D\\times K}\\,$ que escoge PCA a partir de la SVD de $\\mathbf{X},\\,$ $\\mathbf{X}=\\mathbf{U}\\mathbf{S}\\mathbf{V}^t,\\,$ sabiendo que\n",
    "$$\\mathbf{S}=\\operatorname{diag}(\\sqrt{18}, 2)\\quad\\text{y}\\quad\\mathbf{V}^t=\\begin{pmatrix}\\frac{\\sqrt{2}}{2}&\\frac{\\sqrt{2}}{2}\\\\\\frac{\\sqrt{2}}{2}&-\\frac{\\sqrt{2}}{2}\\end{pmatrix}$$\n",
    "\n",
    "<details><summary>Solución:</summary>\n",
    "\n",
    "Escogemos la fila de $\\,\\mathbf{V}^t\\,$ correspondiente al mayor valor singular: $\\;\\mathbf{W}=\\begin{pmatrix}\\frac{\\sqrt{2}}{2}\\\\\\frac{\\sqrt{2}}{2}\\end{pmatrix}$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cálculo:** $\\;$ halla la derivada de $\\;\\sigma(\\beta x)=\\dfrac{1}{1+e^{-\\beta x}},\\;$ donde $\\beta$ es una constante no negativa\n",
    "\n",
    "<details><summary>Solución:</summary>\n",
    "\n",
    "$$\\frac{\\partial\\sigma(\\beta x)}{\\partial x}%\n",
    "=\\frac{\\beta e^{-\\beta x}}{(1+e^{-\\beta x})^2}%\n",
    "=\\beta\\,\\frac{1+e^{-\\beta x}-1}{(1+e^{-\\beta x})^2}%\n",
    "%=\\frac{\\beta}{1+e^{-\\beta x}}-\\frac{\\beta}{(1+e^{-\\beta x})^2}%\n",
    "=\\frac{\\beta}{1+e^{-\\beta x}}\\left(1-\\frac{1}{1+e^{-\\beta x}}\\right)%\n",
    "=\\beta\\sigma(\\beta x)(1-\\sigma(\\beta x))%\n",
    "=\\beta\\sigma(\\beta x)\\sigma(-\\beta x)$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cálculo:** $\\;$ halla la derivada de $\\;\\operatorname{swish}(x)=x\\,\\sigma(\\beta x)=\\dfrac{x}{1+e^{-\\beta x}},\\;$ donde $\\beta$ es una constante no negativa\n",
    "\n",
    "<details><summary>Solución:</summary>\n",
    "\n",
    "$$\\frac{\\partial\\operatorname{swish}(x)}{\\partial x}%\n",
    "=\\sigma(\\beta x)+x\\beta\\sigma(\\beta x)(1-\\sigma(\\beta x))%\n",
    "=\\beta x\\,\\sigma(\\beta x) + \\sigma(\\beta x)(1-\\beta x\\,\\sigma(\\beta x))%\n",
    "=\\beta\\operatorname{swish}(x) + \\sigma(\\beta x)(1-\\beta\\operatorname{swish}(x))$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimización:** $\\;$ La estimación de parámetros en aprendizaje automático requiere resolver un **problema de optimización** que por lo general se plantea en términos de **minimización.** La función objetivo, $\\,\\mathcal{L}(\\boldsymbol{\\theta}),\\,$  toma un vector de $D$ parámetros, $\\,\\boldsymbol{\\theta}\\in\\mathbb{R}^D,\\,$ y devuelve la **pérdida o coste** que se deriva de utilizar $\\boldsymbol{\\theta}$ como estimador puntual de los parámetros. En relación con la dificultad del problema de optimización, indica cuál de las siguientes afirmaciones es incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. Optimización global es por lo general más difícil que optimización local, si bien en ocasiones se puede garantizar que todo óptimo local es también global.\n",
    "2. Optimización con restricciones suele ser más difícil que optimización sin restricciones. No obstante, muchas veces podemos relajar (eliminar) restricciones, resolver el problema relajado y limitarnos a comprobar que la solución hallada cumple las restricciones relajadas.\n",
    "3. En general, $\\,\\boldsymbol{\\theta}^*\\in\\mathbb{R}^D\\,$ es mínimo local si el gradiente del objetivo en $\\,\\boldsymbol{\\theta}^*$ es nulo.\n",
    "4. Las tres opciones anteriores son correctas.\n",
    "\n",
    "<details><summary>Solución:</summary><br>\n",
    "\n",
    "La 3; el gradiente nulo en $\\,\\boldsymbol{\\theta}^*$ es condición necesaria pero no suficiente.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimización:** $\\;$ Dada la función objetivo $\\mathcal{L}(\\theta)=\\theta^4,\\,$ aplica una iteración de descenso por gradiente a partir de $\\,\\theta_0=1\\,$ con factor de aprendizaje $\\,\\eta=0.1$.\n",
    "\n",
    "<details><summary>Solución:</summary><br>\n",
    "\n",
    "$$\\frac{d\\mathcal{L}(\\theta)}{d\\theta}=4\\,\\theta^3\\quad\\to\\quad\\theta_1=\\theta_0-0.1\\cdot 4\\cdot \\theta_0^3=1-0.4\\cdot 1^3=0.6$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probabilidad:** $\\;$ La función de densidad de la Gaussiana $D$-dimensional es de la forma\n",
    "$$p(\\boldsymbol{y}\\mid\\boldsymbol{\\mu},\\mathbf{\\Sigma})=\\dfrac{1}{(2\\pi)^{D/2}\\det{\\mathbf{\\Sigma}}^{1/2}}%\n",
    "\\exp\\left[-\\dfrac{1}{2}(\\boldsymbol{y}-\\boldsymbol{\\mu})^t\\mathbf{\\Sigma}^{-1}(\\boldsymbol{y}-\\boldsymbol{\\mu})\\right], %\n",
    "\\quad\\boldsymbol{\\mu}\\in\\mathbb{R}^D,\\;\\mathbf{\\Sigma}\\in\\mathbb{R}^{D\\times D},\\;\\mathbf{\\Sigma}\\succeq 0$$\n",
    "Sus conjuntos de nivel son proporcionales a los de la distancia de Mahalanobis entre $\\boldsymbol{y}$ y $\\boldsymbol{\\mu}$. La forma de estos conjuntos, esto es, de la bola Gaussiana centrada en $\\boldsymbol{\\mu}$, depende de $\\mathbf{\\Sigma}$. Suponiendo que $D=2$, indica la respuesta incorrecta (o la última opción si las tres primeras son correctas).\n",
    "1. Si $\\mathbf{\\Sigma}=\\sigma^2\\mathbf{I}$, la Gaussiana es circular.\n",
    "2. Si $\\mathbf{\\Sigma}$ es diagonal, la Gaussiana es rectangular.\n",
    "3. Si $\\mathbf{\\Sigma}$ no es diagonal (con elementos no nulos fuera de la diagonal), la Gaussiana es elípitica.\n",
    "4. Las tres opciones anteriores son correctas.\n",
    "\n",
    "<details><summary>Solución:</summary><br>\n",
    "\n",
    "La 2; si $\\mathbf{\\Sigma}$ es diagonal, la Gaussiana es elípitica, de semiejes alineados con los canónicos.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estadística:** $\\;$ Sea $\\,\\mathcal{D}=\\{(\\boldsymbol{x}_n,\\boldsymbol{y}_n)\\}_{n=1}^N\\,$ un conjunto de datos y sea $\\,p(\\mathcal{D}\\mid\\boldsymbol{\\theta})\\,$ la verosimilitud de un modelo gobernado por un vector de parámetros $\\,\\boldsymbol{\\theta}$. Supón que la verosimilitud puede factorizarse de manera naive como $p(\\mathcal{D}\\mid\\boldsymbol{\\theta})=\\prod_n p(\\boldsymbol{y}_n\\mid\\boldsymbol{x}_n,\\boldsymbol{\\theta})$. Con base en esta asunción, se quiere hallar un estimador máximo-verosímil de $\\boldsymbol{\\theta}$, $\\hat{\\boldsymbol{\\theta}}_{\\text{mle}}$. Indica la respuesta incorrecta (o la última opción si las tres primeras son correctas).\n",
    "1. $\\hat{\\boldsymbol{\\theta}}_{\\text{mle}}=\\operatorname{argmin}_{\\boldsymbol{\\theta}}-\\sum_n \\log p(\\boldsymbol{y}_n\\mid\\boldsymbol{x}_n,\\boldsymbol{\\theta})$\n",
    "2. $\\hat{\\boldsymbol{\\theta}}_{\\text{mle}}=\\operatorname{argmin}_{\\boldsymbol{\\theta}}-\\sum_n \\log p(\\boldsymbol{x}_n, \\boldsymbol{y}_n\\mid\\boldsymbol{\\theta})-\\log p(\\boldsymbol{x}_n\\mid\\boldsymbol{\\theta})$\n",
    "3. $\\hat{\\boldsymbol{\\theta}}_{\\text{mle}}=\\operatorname{argmin}_{\\boldsymbol{\\theta}}-\\sum_n \\log p(\\boldsymbol{x}_n, \\boldsymbol{y}_n\\mid\\boldsymbol{\\theta})$\n",
    "4. Las tres opciones anteriores son correctas.\n",
    "\n",
    "<details><summary>Solución:</summary><br>La 3.</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estadística:** $\\;$ Uno de los grandes retos del aprendizaje automático consiste en aprender modelos que generalicen bien, esto es, que no se limiten a predecir correctamente los datos de entrenamiento, sino que también predigan bien datos futuros. Con el fin de afrontar este reto, se suelen seguir varias estrategias convencionales que por lo general dan buen resultado. Indica cuál de las siguientes estrategias **no** es una estrategia convencional para generalizar mejor (o escoge la última opción si las tres primeras sí lo son).\n",
    "1. Regularización.\n",
    "2. Terminación temprana (asumiendo que se usa un algoritmo de aprendizaje iterativo).\n",
    "3. Uso de más datos.\n",
    "4. Las tres opciones anteriores son estrategias convencionales para generalizar mejor.\n",
    "\n",
    "<details><summary>Solución:</summary><br>La 4.</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decisión:** $\\;$ En un problema de clasificación en $C$ clases, tanto los posibles estados de la naturaleza como las posibles acciones son etiquetas de clase, $\\,\\mathcal{H}=\\mathcal{A}=\\mathcal{Y}=\\{1,\\dotsc,C\\}$. Supón que la pérdida asociada a cada par estado-acción es la 01, $\\,\\ell(y^*,\\hat{y})=\\mathbb{I}(y^*\\neq\\hat{y}),\\,$ donde $\\,y^*\\,$ denota la clase verdadera y $\\hat{y}$ la predicha. Dada una observación $\\boldsymbol{x}$, el riesgo de una acción $\\,\\hat{y}\\,$ y la acción de mínimo riesgo (regla de Bayes) son:\n",
    "1. $R(\\hat{y}\\mid\\boldsymbol{x})=1-p(y^*=\\hat{y}\\mid\\boldsymbol{x})\\;$ y $\\;\\pi^*(\\boldsymbol{x})=\\operatorname{argmin}_{y\\in\\mathcal{Y}}\\;p(y\\mid\\boldsymbol{x})$\n",
    "2. $R(\\hat{y}\\mid\\boldsymbol{x})=1-p(y^*=\\hat{y}\\mid\\boldsymbol{x})\\;$ y $\\;\\pi^*(\\boldsymbol{x})=\\operatorname{argmax}_{y\\in\\mathcal{Y}}\\;p(y\\mid\\boldsymbol{x})$\n",
    "3. $R(\\hat{y}\\mid\\boldsymbol{x})=1-p(y^*\\neq\\hat{y}\\mid\\boldsymbol{x})\\;$ y $\\;\\pi^*(\\boldsymbol{x})=\\operatorname{argmin}_{y\\in\\mathcal{Y}}\\;p(y\\mid\\boldsymbol{x})$\n",
    "4. $R(\\hat{y}\\mid\\boldsymbol{x})=1-p(y^*\\neq\\hat{y}\\mid\\boldsymbol{x})\\;$ y $\\;\\pi^*(\\boldsymbol{x})=\\operatorname{argmax}_{y\\in\\mathcal{Y}}\\;p(y\\mid\\boldsymbol{x})$\n",
    "\n",
    "<details><summary>Solución:</summary><br>La 2.</details>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
