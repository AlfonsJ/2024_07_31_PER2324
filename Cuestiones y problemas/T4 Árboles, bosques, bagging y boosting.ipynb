{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T4 Árboles, bosques, bagging y boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CART:** $\\;$ Sea un problema de clasificación de datos 2d en $C=2$ clases, $y\\in\\{1,2\\}$, para el que se está construyendo un árbol de clasificación. El algoritmo de aprendizaje se halla procesando un nodo $i$ cuyo conjunto de datos, $\\mathcal{D}_i$, consta de $4$ datos de la clase $1$, $\\{(1,1)^t,$ $(1,2)^t,$ $(3,1)^t,$ $(3,2)^t\\}$, y $2$ de la clase $2$, $\\{(2,1)^t,$ $(2,2)^t\\}$. Determina la impureza del nodo $i$, así como un split óptimo del mismo en términos de reducción de impureza. Emplea el índice Gini (error de clasificación esperado) para medir la impureza.\n",
    "\n",
    "<details><summary>Solución:</summary>\n",
    "\n",
    "$$G_i=1-\\hat{\\pi}_{i1}^2-\\hat{\\pi}_{i2}^2=1-(4/6)^2-(2/6)^2=1-20/36=16/36=4/9=0.44$$\n",
    "\n",
    "En el eje horizontal (dimensión $d=1$) tenemos dos posibles splits que dejan dos datos de la clase $1$ a un lado y el resto al otro lado, por lo que producen la misma reducción de impureza. Tomemos, por ejemplo, el split basado en el umbral $t=1.5$. En este caso, dos datos de la clase $1$ van al hijo izquierdo y el resto al derecho. Las impurezas de los hijos y reducción de impureza respecto al padre son:\n",
    "\\begin{align*}\n",
    "G(\\mathcal{D}_i^L(1,1.5))&=1-(2/2)^2-(0/2)^2=0\\\\%\n",
    "G(\\mathcal{D}_i^R(1,1.5))&=1-(2/4)^2-(2/4)^2=1/2\\\\%\n",
    "\\Delta&=G_i-2/6\\,G(\\mathcal{D}_i^L(1,1.5))-4/6\\,G(\\mathcal{D}_i^R(1,1.5))\\\\%\n",
    "  &=4/9-2/6\\cdot 0-4/6\\cdot 1/2%=4/9-1/3%\n",
    "    =1/9=0.11\n",
    "\\end{align*}\n",
    "\n",
    "En el eje vertical (dimensión $d=2$) tenemos un único split que deja ambos hijos con dos datos de la clase $1$ y uno de la $2$:\n",
    "\\begin{align*}\n",
    "G(\\mathcal{D}_i^L(1,1.5))&=1-(2/3)^2-(1/3)^2=1-5/9=4/9\\\\%\n",
    "G(\\mathcal{D}_i^R(1,1.5))&=G(\\mathcal{D}_i^L(1,1.5))\\\\%\n",
    "\\Delta&=G_i-3/6\\,G(\\mathcal{D}_i^L(1,1.5))-3/6\\,G(\\mathcal{D}_i^R(1,1.5))\\\\%\n",
    "  &=4/9-3/6\\cdot 4/9-3/6\\cdot 4/9=0\n",
    "\\end{align*}\n",
    "Por tanto, un split óptimo consiste en particionar los datos con la primera variable (horizontal) y umbral $t=1.5$; también sería óptimo emplear el umbral $t=2.5$.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CART:** Sea un problema de clasificación de datos 3d en $C=2$ clases, $y\\in\\{1,2\\}$, para el que se está construyendo un árbol de clasificación. El algoritmo de aprendizaje se halla procesando un nodo $i$ cuyo conjunto de datos, $\\mathcal{D}_i$, consta de $2$ datos de la clase $1$, $\\{(1,1,2)^t,(1,2,1)^t\\}$, y $2$ de la clase $2$, $\\{(2,1,2)^t,(2,2,1)^t\\}$. Determina la impureza del nodo $i$, así como un split óptimo del mismo en términos de reducción de impureza. Emplea el índice Gini (error de clasificación esperado) para medir la impureza.\n",
    "\n",
    "<details><summary>Solución:</summary>\n",
    "\n",
    "El nodo $i$ tiene dos datos de la clase $1$ y otros de la $2$, por lo que resulta máximamente impuro:\n",
    "$$G_i=1-\\hat{\\pi}_{i1}^2-\\hat{\\pi}_{i2}^2=1-(2/4)^2-(2/4)^2=1-1/2=1/2$$\n",
    "\n",
    "Es fácil determinar los posibles splits del nodo $i$ si tenemos en cuenta que, en las tres dimensiones, los datos están en $1$ y $2$ únicamente. Por tanto, solo cabe un split por cada dimensión, en $1.5$ por ejemplo. El split en la dimensión $1$ deja los dos datos de la clase $1$ a un lado y los dos datos de la clase $2$ al otro; esto es, produce dos nodos hijos puros. Por el contratrio, los splits en las dimensiones $2$ y $3$ producen hijos con un dato de cada clase; esto es, máximamente impuros, como el nodo $i$. En definitiva, el split de la dimensión $1$ es óptimo y produce la máxima reducción de impureza posible:\n",
    "\\begin{align*}\n",
    "G(\\mathcal{D}_i^L(1,1.5))&=1-(2/2)^2-(0/2)^2=0\\\\[3mm]%\n",
    "G(\\mathcal{D}_i^R(1,1.5))&=1-(0/2)^2-(2/2)^2=0\\\\[3mm]%\n",
    "\\Delta&=G_i-2/4\\,G(\\mathcal{D}_i^L(1,1.5))-2/4\\,G(\\mathcal{D}_i^R(1,1.5))\\\\%\n",
    "  &=1/2-0-0=1/2\n",
    "\\end{align*}\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CART:** Los árboles presentan una serie de ventajas e incovenientes que cabe tener presentes para su uso. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. Son insensibles a transformaciones monótonas de las entradas ya que los puntos de split se basan en la ordenación de los datos, por lo que es necesario estandarizarlos.\n",
    "2. Son de ajuste rápido y fácil escalado a muchos datos.\n",
    "3. Son inestables, esto es, pequeños cambios en los datos de entrada pueden conducir a grandes cambios en la estructura del árbol.\n",
    "4. Todas son correctas.\n",
    "\n",
    "<details><summary>Solución:</summary><br>La 1 es incorrecta. Al revés de lo que dice, no es necesario estandarizar los datos porque los árboles son insensibles a transformaciones monótonas de las entradas.</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient boosting:** $\\;$ Gradient boosting es una técnica **forward stagewise additive modeling (FSAM),** pero abandona la idea de reponderar datos iterativamente y replantea FSAM como descenso por gradiente para un problema de búsqueda en un espacio funcional, $\\hat{\\boldsymbol{f}}=\\operatorname{argmin}_{\\boldsymbol{f}}\\mathcal{L}(\\boldsymbol{f})$. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. El paso $m$ halla $\\boldsymbol{f}_m=\\boldsymbol{f}_{m-1}-\\beta_m\\boldsymbol{g}_m$, donde $\\boldsymbol{g}_m$ es el gradiente de $\\mathcal{L}(\\boldsymbol{f})$ en $\\boldsymbol{f}=\\boldsymbol{f}_{m-1}$ y $\\beta_m$ se obtiene por búsqueda lineal.\n",
    "2. $\\boldsymbol{f}_m$ se obtiene añadiendo a $\\boldsymbol{f}_{m-1}$ un modelo base $F_m(\\boldsymbol{x})$ ajustado por mínimos cuadrados al residuo del gradiente.\n",
    "3. $\\beta_m$ suele sustituirse por un factor de reducción $0<\\nu\\leq 1$ que permite regularizar y, así, $f_m(\\boldsymbol{x})=f_{m-1}(\\boldsymbol{x})+\\nu F_m(\\boldsymbol{x})$.\n",
    "4. Todas son correctas.\n",
    "\n",
    "<details><summary>Solución:</summary><br>La 4.</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparación:** $\\;$ Los árboles constituyen un estimador de alta varianza ya que pequeñas perturbaciones de los datos de entrenamiento resultan en predicciones muy distintas. El aprendizaje de ensambles reduce la varianza de los árboles promediando $\\lvert\\mathcal{M}\\rvert$ modelos base $\\{f_m\\}$, $f(y\\mid\\boldsymbol{x})=\\frac{1}{\\lvert\\mathcal{M}\\rvert}\\sum\\nolimits_{m\\in\\mathcal{M}}f_m(y\\mid\\boldsymbol{x})$. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. En regresión, el promediado suele presentar un sesgo similar al de los modelos base, pero mejor precisión por la menor varianza.\n",
    "2. En clasificación se usa el voto mayoritario o método comité, que incrementa la precisión de los modelos base, especialmente cuando sus errores de predicción no se hallan correlados.\n",
    "3. Bagging, bosques aleatorios y gradient tree boosting aprenden ensambles de árboles con el fin de reducir la varianza. Además, bosques aleatorios y gradient tree boosting adaptan el ajuste de cada modelo base teniendo en cuenta los modelos base ajustados previamente. De esta manera, no solo consiguen reducir la varianza, sino que también reducen el sesgo del ensamble resultante.\n",
    "4. Todas son correctas.\n",
    "\n",
    "<details><summary>Solución:</summary><br>La 3; bosques aleatorios no reduce el sesgo.</details>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
