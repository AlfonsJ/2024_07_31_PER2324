{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T3 Modelos lineales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generativos vs discriminativos:** $\\;$ En relación con las ventajas de los clasificadores generativos y discriminativos, indica la afirmación incorrecta:\n",
    "1. Los generativos son, por lo general, más fáciles de ajustar.\n",
    "2. Los discriminativos suelen ofrecer mejor precisión que los generativos.\n",
    "3. Los generativos son más flexibles en preproceso de características.\n",
    "4. Los generativos facilitan el tratamiento de datos perdidos.\n",
    "\n",
    "<details><summary>Solución:</summary><br>La 3; los discriminativos son más flexibles en preproceso de características.</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QDA y LDA:** $\\;$ Indica la afirmación incorrecta (o escoge la última opción si las tres primeras son correctas):\n",
    "1. Análisis discriminante cuadrático (QDA) asume que las densidades condicionales de las clases son Gaussianas independientes, lo que resulta en discriminantes (log-posteriors) cuadráticas.\n",
    "2. Análisis discriminante lineal (LDA) asume que las densidades condicionales de las clases son Gaussianas de matriz de covarianzas común, lo que resulta en discriminantes (log-posteriors) lineales.\n",
    "3. LDA suele ajustarse con las medias empíricas de las clases y la matriz de covarianzas empírica global.\n",
    "4. Las tres afirmaciones anteriores son correctas.\n",
    "\n",
    "<details><summary>Solución:</summary><br>La 3. QDA suele ajustarse con las medias y matrices de covarianzas empíricas de las clases. LDA suele ajustarse con las medias empíricas de las clases y la matriz de covarianzas empírica global.</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes (NB):** $\\;$ El clasificador NB Bernoulli asume que las funciones de probabilidad de las clases cumplen la asunción NB,\n",
    "$$p(\\boldsymbol{x}\\mid y=c,\\boldsymbol{\\theta}_c)=\\prod_{d=1}^D \\operatorname{Ber}(x_d\\mid\\theta_{cd})\n",
    "\\quad\\text{con}\\quad\\boldsymbol{x}=(x_1,\\dotsc,x_D)^t\\in\\{0,1\\}^D\\quad\\text{y}\\quad\\boldsymbol{\\theta}_c=(\\theta_{c1},\\dotsc,\\theta_{cD})^t\\in[0,1]^D$$\n",
    "Nótese que $\\theta_{cd}$ es la probabilidad de que $x_d=1$ en la clase $c$. Supón que $\\,C=2,\\,$ $p(y=1)=p(y=2)=0.5,\\,$, $D=2,\\,$ $\\boldsymbol{\\theta}_1=(0.7, 0.3)^t\\,$ y $\\,\\boldsymbol{\\theta}_2=(0.2, 0.8)^t$. Determina la probabilidad a posteriori de que la muestra $\\;\\boldsymbol{x}=(0,1)^t$ pertenezca a la clase $1$.\n",
    "\n",
    "<details><summary>Solución:</summary><br>\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\boldsymbol{x}\\mid y=1;\\boldsymbol{\\theta})%\n",
    "&=\\operatorname{Ber}(x_1\\mid 0.7)\\,\\operatorname{Ber}(x_2\\mid 0.3)=0.3\\cdot 0.3=0.09\\\\%\n",
    "p(\\boldsymbol{x}\\mid y=2;\\boldsymbol{\\theta})%\n",
    "&=\\operatorname{Ber}(x_1\\mid 0.2)\\,\\operatorname{Ber}(x_2\\mid 0.8)=0.8\\cdot 0.8=0.64\\\\%\n",
    "p(y=1\\mid\\boldsymbol{x};\\boldsymbol{\\theta})%\n",
    "&=\\frac{p(\\boldsymbol{x}\\mid y=1;\\boldsymbol{\\theta})}\n",
    "{p(\\boldsymbol{x}\\mid y=1;\\boldsymbol{\\theta})+p(\\boldsymbol{x}\\mid y=2;\\boldsymbol{\\theta})}%\n",
    "&&(\\text{priors equiprobables})\\\\%\n",
    "&=\\frac{0.09}{0.09+0.64}=12.3\\%\n",
    "\\end{align*}$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regresión logística:** $\\;$ Regresión logística es un modelo de clasificación $p(y\\mid\\boldsymbol{x};\\boldsymbol{\\theta})$, donde $\\boldsymbol{x}\\in\\mathbb{R}^D$ es un vector de $D$ características, $y\\in\\{1,\\dotsc,C\\}$ es la etiqueta de clase y $\\boldsymbol{\\theta}$ es un vector de parámetros. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas):\n",
    "1. Si $C=2$, el modelo es de regresión logística binaria.\n",
    "2. Si $C>2$, el modelo es de regresión logística multinomial.\n",
    "3. Si $C>2$, el modelo es de regresión logística multiclase.\n",
    "4. Las tres afirmaciones anteriores son correctas.\n",
    "\n",
    "<details><summary>Solución:</summary><br>La 4.</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regresión logística binaria:** $\\;$ Sea un problema de clasificación en dos clases, $y\\in\\{0,1\\}$, y sea $p(y\\mid\\boldsymbol{x};\\boldsymbol{\\theta})=\\operatorname{Ber}(y\\mid\\sigma(\\boldsymbol{w}^t\\boldsymbol{x}+b))$ un modelo de regresión logística binaria con $\\boldsymbol{w}=(3,3)^t$ y $b=0$. Determina la probabilidad de que $\\boldsymbol{x}=(0.5,0.5)^t$ pertenezca a la clase $1$ según el modelo dado.\n",
    "\n",
    "<details><summary>Solución:</summary><br>\n",
    "\n",
    "$$p(y=1\\mid\\boldsymbol{x};\\boldsymbol{\\theta})=\\sigma(\\boldsymbol{w}^t\\boldsymbol{x}+b)=\\sigma((3, 3)(0.5, 0.5)^t+0)=\\dfrac{1}{1+e^{-3}}=0.9526$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regresión logística binaria:** $\\;$ Sea un problema de clasificación en dos clases, $y\\in\\{0,1\\}$, y sea $p(y\\mid\\boldsymbol{x};\\boldsymbol{\\theta})=\\operatorname{Ber}(y\\mid\\sigma(\\boldsymbol{w}^t\\boldsymbol{x}+b))$ un modelo de regresión logística binaria con $\\boldsymbol{w}=(3,3)^t$ y $b=0$. Determina la logit, $f(\\boldsymbol{x}; \\boldsymbol{\\theta})$, así como la frontera y regiones de decisión que induce (con pérdida 0-1).\n",
    "\n",
    "<details><summary>Solución:</summary><br>\n",
    "\n",
    "Logit: $\\quad f(\\boldsymbol{x}; \\boldsymbol{\\theta})=b+\\boldsymbol{w}^t\\boldsymbol{x}=3 x_1+3 x_2$\n",
    "\n",
    "Frontera: $\\quad 3 x_1+3 x_2=0\\to x_2=-x_1$\n",
    "\n",
    "Regiones de decisión:\n",
    "$$\\begin{align*}\n",
    "  \\mathcal{R}_1%\n",
    "  &=\\{\\boldsymbol{x}:p(y=1\\mid\\boldsymbol{x};\\boldsymbol{\\theta})>p(y=0\\mid\\boldsymbol{x};\\boldsymbol{\\theta})\\}\\\\%\n",
    "  &=\\left\\{\\boldsymbol{x}:\\frac{p(y=1\\mid\\boldsymbol{x};\\boldsymbol{\\theta})}{p(y=0\\mid\\boldsymbol{x};\\boldsymbol{\\theta})}>1\\right\\}\\\\%\n",
    "  &=\\{\\boldsymbol{x}:f(\\boldsymbol{x};\\boldsymbol{\\theta})>0\\}\\\\%\n",
    "  &=\\{\\boldsymbol{x}:3 x_1+3 x_2>0\\}\\\\%\n",
    "  &=\\{\\boldsymbol{x}:x_2>-x_1\\}\\\\[3mm]%\n",
    "  \\mathcal{R}_0%\n",
    "  &=\\{\\boldsymbol{x}:x_2<-x_1\\}%\n",
    "\\end{align*}$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regresión logística binaria:** $\\;$ Sea $\\mathcal{D}=\\{(\\boldsymbol{x}_n,y_n): \\boldsymbol{x}_n\\in\\mathbb{R}^D, y_n\\in\\{0,1\\}\\}$, un conjunto de muestras no linealmente separables. Se quiere aprender un modelo de clasificación $p(y\\mid\\boldsymbol{x};\\boldsymbol{\\theta})$ que las clasifique sin error (pérdida 0-1 nula). Indica la respuesta correcta:\n",
    "1. Podemos usar regresión logística binaria convencional, esto es, con $p(y=1\\mid\\boldsymbol{x})=\\operatorname{Ber}(y\\mid\\sigma(\\boldsymbol{w}^t\\boldsymbol{x}+b))$.\n",
    "2. No podemos usar regresión logística binaria convencional, pero sí regresión logística binaria con una función de preproceso, $\\phi(\\boldsymbol{x})$, que linearice las muestras.\n",
    "3. No podemos usar regresión logística binaria, convencional o no, pues necesitamos un modelo de clasificación multi-clase.\n",
    "4. No podemos usar regresión logística binaria, convencional o no, pues es un modelo lineal y necesitamos uno no lineal.\n",
    "\n",
    "<details><summary>Solución:</summary><br>La 2; la 4 es falsa porque puede haber una función de preproceso que linearice las muestras.</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regresión logística binaria:** $\\;$ Sea $\\,p(y\\mid\\boldsymbol{x};\\boldsymbol{w})=\\operatorname{Ber}(y\\mid\\sigma(\\boldsymbol{w}^t\\boldsymbol{x})),\\,$ $y\\in\\{0,1\\},\\,$ un modelo de regresión logística binaria en notación compacta (homogénea). Sea el conjunto de datos de entrenamiento $\\,\\mathcal{D}=\\{(\\boldsymbol{x}_n,y_n)\\},\\,$ donde $\\,\\boldsymbol{x}_n\\in\\mathbb{R}^D\\,$ y $\\,y_n\\in\\{0,1\\}$. Sabemos que la derivada de la NLL es:\n",
    "$$\\frac{\\partial\\mathcal{L}(\\boldsymbol{w})}{\\partial\\boldsymbol{w}}=\\frac{1}{N}\\sum_{n=1}^N(\\mu_n-y_n)\\,\\boldsymbol{x}_n%\n",
    "\\quad\\text{donde}\\quad\\mu_n=\\sigma(a_n)\\quad\\text{con log-odds}\\quad a_n=\\boldsymbol{w}^t\\boldsymbol{x}_n$$\n",
    "Supón que estamos aplicando SGD con minibatch de talla $1$ para minimizar la $\\operatorname{NLL}$. Más concretamente, nos hallamos en la iteración $i$ con $\\boldsymbol{w}_i=(-1, 0, -1)^t,\\,$ $\\eta_i=0.1\\,$ y el minibatch actual únicamente incluye la muestra $\\,\\boldsymbol{x}_n=(1, 1, 1)^t,\\,$ de clase $\\,y_n=1$. Determina $\\,\\boldsymbol{w}_{i+1}$.\n",
    "\n",
    "<details><summary>Solución:</summary><br>\n",
    "\n",
    "$$p(y_n=1\\mid\\boldsymbol{x}_n;\\boldsymbol{w}_i)=\\mu_n=\\sigma(\\boldsymbol{w}_t^t\\boldsymbol{x}_n)=\\sigma(-2)=\\dfrac{1}{1+e^2}=0.12$$\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{w}_{i+1}%\n",
    "&=\\boldsymbol{w}_i-\\eta_i\\,(\\mu_n-y_n)\\boldsymbol{x}_n\\\\%\n",
    "&=(-1, 0, -1)^t+0.1\\,(1-0.12)(1, 1, 1)^t\\\\%\n",
    "&=(-1, 0, -1)^t+0.09\\,(1, 1, 1)^t\\\\%\n",
    "&=(-0.91, 0.09, -0.91)^t%\n",
    "\\end{align*}$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptrón:** $\\;$  Sea $\\,p(y\\mid\\boldsymbol{x};\\boldsymbol{w})=\\operatorname{Ber}(y\\mid H(\\boldsymbol{w}^t\\boldsymbol{x})),\\,$ $y\\in\\{0,1\\},\\,$ un Perceptrón binario en notación homogénea ($x_0=1\\,$ y $\\,b=w_{i0}$). Supón que estamos aplicando el algoritmo Perceptrón para actualizar el vector de parámetros actual, $\\,\\boldsymbol{w}_i,\\,$ con una muestra $\\,(\\boldsymbol{x}_n,y_n)\\,$ factor de aprendizaje $\\,\\eta_i=1$. Si $\\,y_n=0\\,$ y $\\,\\hat{y}_n=\\mathbb{I}(\\boldsymbol{w}_i^t\\boldsymbol{x}>0)=1,\\,$ entonces:\n",
    "1. $\\boldsymbol{w}_{i+1}=\\boldsymbol{w}_i$\n",
    "2. $\\boldsymbol{w}_{i+1}=\\boldsymbol{w}_i+\\boldsymbol{x}_n$\n",
    "3. $\\boldsymbol{w}_{i+1}=\\boldsymbol{w}_i-\\boldsymbol{x}_n$\n",
    "4. Ninguna de las anteriores.\n",
    "\n",
    "<details><summary>Solución:</summary><br>La 3.</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptrón:** $\\;$ El algoritmo Perceptrón puede verse como una variante de SGD para regresión logística binaria con minibatch de talla $1$ y pseudo-derivada de la NLL \n",
    "$$\\frac{\\partial\\mathcal{L}(\\boldsymbol{w})}{\\partial\\boldsymbol{w}}=(\\hat{y}_n-y_n)\\,\\boldsymbol{x}_n%\n",
    "\\quad\\text{donde}\\quad\\hat{y}_n=H(a_n)\\quad\\text{con log-odds}\\quad a_n=\\boldsymbol{w}^t\\boldsymbol{x}_n$$\n",
    "Supón que nos hallamos en la iteración $i$ con $\\,\\boldsymbol{w}_i=(-1, 0, -1)^t,\\,$ $\\eta_i=0.1\\,$ y muestra actual $\\,\\boldsymbol{x}_n=(1, 1, 1)^t,\\,$ de clase $\\,y_n=1$. Determina $\\,\\boldsymbol{w}_{i+1}$.\n",
    "\n",
    "<details><summary>Solución:</summary><br>\n",
    "\n",
    "$$p(y_n=1\\mid\\boldsymbol{x}_n;\\boldsymbol{w}_i)=H(\\boldsymbol{w}_i^t\\boldsymbol{x}_n)=H(-2)=0\\quad\\to\\quad\\hat{y}_n=0$$\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{w}_{i+1}%\n",
    "&=\\boldsymbol{w}_i-\\eta_i\\,(\\hat{y}_n-y_n)\\boldsymbol{x}_n\\\\%\n",
    "&=(-1, 0, -1)^t+0.1\\,(1-0)(1, 1, 1)^t\\\\%\n",
    "&=(-1, 0, -1)^t+0.1\\,(1, 1, 1)^t\\\\%\n",
    "&=(-0.9, 0.1, -0.9)^t%\n",
    "\\end{align*}$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regresión logística (multiclase):** $\\;$ Sea un problema de clasificación en $\\,C=3\\,$ clases, $\\,y\\in\\{1,2,3\\},\\,$ y sea el modelo de regresión logística\n",
    "$$p(y\\mid\\boldsymbol{x};\\mathbf{W})=\\operatorname{Cat}(y\\mid\\mathcal{S}(\\mathbf{W}^t\\boldsymbol{x}))%\n",
    "\\quad\\text{con}\\quad\n",
    "\\mathbf{W}=\\begin{pmatrix}1&-1&0\\\\1&1&0\\end{pmatrix}$$\n",
    "Determina la probabilidad de que $\\,\\boldsymbol{x}=(0.5, 0.5)^t\\,$ pertenezca a la clase $1$.\n",
    "\n",
    "<details><summary>Solución:</summary><br>\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{a}&=\\mathbf{W}^t\\boldsymbol{x}=\\begin{pmatrix}1&1\\\\-1&1\\\\0&0\\end{pmatrix}\\begin{pmatrix}0.5\\\\0.5\\end{pmatrix}%\n",
    "=\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}\\\\\n",
    "p(y=1\\mid\\boldsymbol{x};\\boldsymbol{\\theta})\n",
    "&=\\mathcal{S}(\\boldsymbol{a})_1%\n",
    "=\\frac{e^1}{e^0+e^0+e^1}=\\frac{e}{1+1+e}=\\frac{1}{1+2/e}=0.5761\n",
    "\\end{align*}$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regresión logística:** $\\;$ Sea un problema de clasificación en $\\,C=3\\,$ clases, $\\,y\\in\\{1,2,\\dotsc,C\\},\\,$ y sea el modelo de regresión logística en notación homogénea\n",
    "$$p(y\\mid\\boldsymbol{x};\\mathbf{W})%\n",
    "=\\operatorname{Cat}(y\\mid \\mathcal{S}(\\mathbf{W}^t\\boldsymbol{x}))\\quad\\text{con}\\quad\\mathbf{W}\\in\\mathbb{R}^{D\\times C}$$\n",
    "Dado un conjunto de datos $\\mathcal{D}=\\{(\\boldsymbol{x}_n,\\boldsymbol{y}_n)\\},\\,\\boldsymbol{x}_n\\in\\mathbb{R}^D,\\,\\boldsymbol{y}_n\\,$ one-hot, la derivada de la NLL es:\n",
    "$$\\frac{\\partial\\mathcal{L}(\\mathbf{W})}{\\partial\\mathbf{W}}%\n",
    "=\\frac{1}{N}\\sum_{n=1}^N\\boldsymbol{x}_n(\\boldsymbol{\\mu}_n-\\boldsymbol{y}_n)^t\n",
    "\\quad\\text{donde}\\quad\\boldsymbol{\\mu}_n=\\mathcal{S}(\\boldsymbol{a}_n)\\quad\\text{con logits}\\quad\\boldsymbol{a}_n=\\mathbf{W}^t\\boldsymbol{x}_n$$\n",
    "Supón que estamos aplicando SGD con minibatch de talla $1$ para minimizar la $\\operatorname{NLL}$. Más concretamente, nos hallamos en la iteración $i$ con \n",
    "la la muestra $\\,\\boldsymbol{x}_n=(1, 1, 1)^t,\\,$ de clase $y_n=1,\\,$ factor de aprendizaje $\\,\\eta_i=0.1\\,$ y\n",
    "$$\\mathbf{W}_i=\\begin{pmatrix}1&0&1\\\\-1&1&-1\\\\0&0&1\\end{pmatrix}$$\n",
    "Determina $\\mathbf{W}_{i+1}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Solución:</summary><br>\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{a}_n%\n",
    "&=\\mathbf{W}_i^t\\boldsymbol{x}_n%\n",
    "=\\begin{pmatrix}1&-1&0\\\\0&1&0\\\\1&-1&1\\end{pmatrix}\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}%\n",
    "=\\begin{pmatrix}0\\\\1\\\\1\\end{pmatrix}\\\\[3mm]%\n",
    "\\boldsymbol{\\mu}_n%\n",
    "&=S(\\boldsymbol{a}_n)%\n",
    "=\\dfrac{1}{1+2e}\\begin{pmatrix}1\\\\e\\\\e\\end{pmatrix}%\n",
    "=\\begin{pmatrix}0.1554\\\\0.4223\\\\0.4223\\end{pmatrix}\\\\[3mm]%\n",
    "\\mathbf{W}_{i+1}%\n",
    "&=\\mathbf{W}_i-\\eta_i\\boldsymbol{x}_n(\\boldsymbol{\\mu}_n-\\boldsymbol{y}_n)^t\\\\%\n",
    "&=\\begin{pmatrix}1&0&1\\\\-1&1&-1\\\\0&0&1\\end{pmatrix}-0.1\\,\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}(-0.8446, 0.4223, 0.4223)\\\\%\n",
    "&=\\begin{pmatrix}1&0&1\\\\-1&1&-1\\\\0&0&1\\end{pmatrix}-\\begin{pmatrix}-0.0845&-0.0845&-0.0845\\\\0.0422&0.0422&0.0422\\\\0.0422&0.0422&0.0422\\end{pmatrix}\\\\%\n",
    "&=\\begin{pmatrix}1.0845&0.0845&1.0845\\\\-1.0422&0.9578&-1.0422\\\\-0.0422&-0.0422&0.9578\\end{pmatrix}\n",
    "\\end{align*}$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.07 Introducción a regresión lineal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.07.01:** Regresión lineal predice una variable dependiente o respuesta, $y\\in\\mathbb{R}$, a partir de variables independientes, explicativas o regresoras, $\\boldsymbol{x}\\in\\mathbb{R}^D$. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas):\n",
    "1. Se asume que el valor esperado de $y$ es función lineal de $\\boldsymbol{x}$.\n",
    "2. Suele asumirse que las variables explicativas incluyen una constante $x_0=1$ para representar el modelo de forma compacta.\n",
    "3. En ocasiones, las variables regresoras son funciones no lineales de los datos pero el modelo sigue considerándose lineal.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
