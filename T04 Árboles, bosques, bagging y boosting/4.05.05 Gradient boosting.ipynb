{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting\n",
    "\n",
    "**Gradient boosting** es una técnica genérica que interpreta FSAM como un algoritmo descenso por gradiente para un problema de minimización en un espacio funcional:\n",
    "$$\\hat{\\boldsymbol{f}}%\n",
    "=\\operatorname*{argmin}_{\\boldsymbol{f}}\\mathcal{L}(\\boldsymbol{f})\n",
    "=\\operatorname*{argmin}_{\\boldsymbol{f}}\\sum_{i=1}^N \\ell(y_i,f(\\boldsymbol{x}_i))$$\n",
    "donde, por simplicidad, las funciones se representan por sus valores en el conjunto de entrenamiento, $\\boldsymbol{f}=(f(\\boldsymbol{x}_1),\\dotsc,f(\\boldsymbol{x}_N))^t$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteración $m$\n",
    "\n",
    "En la iteración $m$ se desea construir un modelo $\\boldsymbol{f}_m$ añadiendo una función base, $F_m$, a un modelo previo, $\\boldsymbol{f}_{m-1}$, tal que $\\mathcal{L}(\\boldsymbol{f}_m)\\leq\\mathcal{L}(\\boldsymbol{f}_{m-1})$. Visto como un problema de minimización en un espacio funcional, descenso por gradiente consiste en escoger la \"dirección\" de máximo descenso, esto es, la del negativo del gradiente de $\\mathcal{L}(\\boldsymbol{f})$ en $\\boldsymbol{f}_{m-1}$, $\\boldsymbol{g}_m$, y construir el nuevo modelo como:\n",
    "$$\\boldsymbol{f}_m=\\boldsymbol{f}_{m-1}-\\beta_m \\boldsymbol{g}_m\n",
    "\\qquad\\text{con}\\qquad%\n",
    "g_{im}=\\left[\\frac{\\partial\\ell(y_i,f(\\boldsymbol{x}_i))}{\\partial f(\\boldsymbol{x}_i)}\\right]_{f_{m-1}(\\boldsymbol{x}_i)}$$\n",
    "donde el factor de aprendizaje puede escogerse por búsqueda lineal. Ahora bien, el algoritmo no es muy útil si se limita a funciones representadas por sus valores en el conjunto de entrenamiento, ya que no podemos generalizar. Para poder hacerlo, gradient boosting ajusta el aprendiz débil al negativo del gradiente:\n",
    "$$F_m=\\operatorname*{argmin}_F\\sum_{i=1}^N(-g_{im}-F(\\boldsymbol{x}_i))^2$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo básico\n",
    "\n",
    "El algoritmo básico prescinde de $\\beta_m$ pero incluye un **shrinkage factor** $0<\\nu\\leq 1$ para facilitar la regularización:\n",
    "1. Inicializar $f_0(\\boldsymbol{x})=\\operatorname{argmin}_F\\sum_{i=1}^N\\ell(y_i,F(\\boldsymbol{x}_i))$\n",
    "2. **for** $\\;m=1:M\\;$ **do**\n",
    "3. $\\quad$ Calcular el negativo del gradiente o (pseudo-)residuo $r_{im}=-\\left[\\dfrac{\\partial\\ell(y_i,f(\\boldsymbol{x}_i))}{\\partial f(\\boldsymbol{x}_i)}\\right]_{f_{m-1}(\\boldsymbol{x}_i)}$\n",
    "4. $\\quad$ Usar el aprendiz débil para hallar $F_m=\\operatorname*{argmin}_F\\sum_{i=1}^N(r_{im}-F(\\boldsymbol{x}_i))^2$\n",
    "5. $\\quad$ Actualizar $f_m(\\boldsymbol{x})=f_{m-1}(\\boldsymbol{x})+\\nu F_m(\\boldsymbol{x})$\n",
    "6. Devolver $f(\\boldsymbol{x})=f_M(\\boldsymbol{x})$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pérdidas y neg-gradientes para regresión\n",
    "\n",
    "En regresión, $y_i\\in\\mathbb{R}$ y se suele usar el error cuadrático como función de pérdida. Más concretamente, se usa la mitad del error cuadrático:\n",
    "$$\\ell(y_i,f(\\boldsymbol{x}_i))=\\frac{1}{2}(y_i-f(\\boldsymbol{x}_i))^2$$\n",
    "pues el neg-gradiente es el residuo de la predicción:\n",
    "$$r_i=y_i-f(\\boldsymbol{x}_i)$$\n",
    "Nótese que gradient boosting con pérdida cuadrática coincide con boosting mínimos cuadrados.\n",
    "\n",
    "Una alternativa al error cuadrático en regresión es el error absoluto:\n",
    "$$\\ell(y_i,f(\\boldsymbol{x}_i))=\\lvert y_i-f(\\boldsymbol{x}_i)\\rvert$$\n",
    "El neg-gradiente es el signo del residuo de la predicción:\n",
    "$$r_i=\\operatorname{sgn}(y_i-f(\\boldsymbol{x}_i))$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pérdidas y neg-gradientes para clasificación binaria\n",
    "\n",
    "En clasificación binaria, $\\tilde{y}_i\\in\\{-1,+1\\}$ y se suele usar la pérdida exponencial:\n",
    "$$\\ell(\\tilde{y}_i,f(\\boldsymbol{x}_i))=\\exp(-\\tilde{y}_i f(\\boldsymbol{x}_i))$$\n",
    "con\n",
    "$$r_i=\\tilde{y}_i\\exp(-\\tilde{y}_i f(\\boldsymbol{x}_i))$$\n",
    "Gradient boosting con pérdida exponencial es prácticamente idéntico a AdaBoost.\n",
    "\n",
    "También se usa la log-pérdida binaria:\n",
    "$$\\ell(\\tilde{y}_i,f(\\boldsymbol{x}_i))=\\log(1+\\exp(-\\tilde{y}_i f(\\boldsymbol{x}_i)))$$\n",
    "con\n",
    "$$\\begin{align*}\n",
    "r_i%\n",
    "&=-\\frac{1}{1+\\exp(-\\tilde{y}_i f(\\boldsymbol{x}_i))}\\exp(-\\tilde{y}_i f(\\boldsymbol{x}_i))(-\\tilde{y}_i)\\\\\n",
    "&=\\tilde{y}_i\\frac{1}{1+\\exp(\\tilde{y}_i f(\\boldsymbol{x}_i))}\\\\\n",
    "&=\\tilde{y}_i\\sigma(-\\tilde{y}_i f(\\boldsymbol{x}_i))\n",
    "\\end{align*}$$\n",
    "Gradient boosting con log-pérdida binaria es prácticamente idéntico a LogitBoost; algo más sencillo de implementar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pérdidas y neg-gradientes para clasificación multiclase\n",
    "\n",
    "En clasificación multiclase, $y_i\\in\\{1,\\dotsc,C\\}$ y se suele usar la log-pérdida. El algoritmo básico se extiende para ajustar $C$ modelos aditivos, uno por cada clase, cuyas predicciones se normalizan mediante una softmax:\n",
    "$$\\ell(y_i,f_1(\\boldsymbol{x}_i),\\dotsc,f_C(\\boldsymbol{x}_i))=-\\sum_c \\mathbb{I}(y_i=c)\\log \\pi_{ic}$$\n",
    "donde\n",
    "$$\\pi_{ic}=S(f_1(\\boldsymbol{x}_i),\\dotsc,f_C(\\boldsymbol{x}_i))_c%\n",
    "=\\frac{\\exp(f_c(\\boldsymbol{x}_i))}{\\sum_{c'=1}^C \\exp(f_{c'}(\\boldsymbol{x}_i))}$$\n",
    "El neg-gradiente para la clase $c$ es:\n",
    "$$\\begin{align*}\n",
    "r_{ic}%\n",
    "&=-\\dfrac{\\partial\\ell(y_i,f_1(\\boldsymbol{x}_i),\\dotsc,f_C(\\boldsymbol{x}_i))}{\\partial f_c(\\boldsymbol{x}_i)}\\\\%\n",
    "&=\\frac{\\partial}{\\partial f_c(\\boldsymbol{x}_i)}\\sum_{\\tilde{c}} \\mathbb{I}(y_i=\\tilde{c})\\log \\pi_{i\\tilde{c}}\\\\%\n",
    "&=\\mathbb{I}(y_i=c)\\frac{1}{\\pi_{ic}}\\pi_{ic}(1-\\pi_{ic})\\\\%\n",
    "&=\\mathbb{I}(y_i=c)(1-\\pi_{ic})\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient tree boosting\n",
    "\n",
    "**Gradient tree boosting** es gradient boosting con árbol de regresión como aprendiz débil:\n",
    "$$F_m=\\sum_{j=1}^{J_m}w_{jm}\\mathbb{I}(\\boldsymbol{x}\\in R_{jm})$$\n",
    "donde $R_{jm}$ y $w_{jm}$ son la región y salida asociadas a la hoja $j$ del árbol añadido en la iteración $m$. La salida puede ser un escalar o, más generalmente, un vector (de probabilidades, por ejemplo).\n",
    "\n",
    "El aprendizaje de las regiones se lleva a cabo mediante CART sobre residuos. La salida de cada hoja se aprende mediante minimización del riesgo empírico con los datos de la hoja:\n",
    "$$\\hat{w}_{jm}=\\operatorname*{argmin}_w\\sum_{\\boldsymbol{x}_i\\in R_{jm}}%\n",
    "\\ell(y_i,f_{m-1}(\\boldsymbol{x}_i)+w)$$\n",
    "En el caso de la pérdida cuadrática, $\\hat{w}_{jm}$ es la media empírica de los residuos de la hoja."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "**Extreme gradient boosting (XGBoost)** es una implementación muy popular de gradient tree boosting con algunos refinamientos adicionales:\n",
    "* Objetivo regularizado\n",
    "* Aproximación de segundo orden de la pérdida\n",
    "* Muestreo de caracterı́sticas en nodos internos\n",
    "* Técnicas informáticas varias para mejorar la escabilidad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
