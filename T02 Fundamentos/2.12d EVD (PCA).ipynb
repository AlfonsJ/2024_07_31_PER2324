{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal component analysis (PCA)\n",
    "\n",
    "Sea $\\mathbf{X}\\in\\mathbb{R}^{N\\times D}$ una matriz de datos y sea $\\mathbf{\\Sigma}=\\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{U}^t$ una descomposición propia de la matriz de covarianzas empírica con valores propios en orden no ascendente, esto es, $\\mathbf{\\Lambda}=\\operatorname{diag}(\\lambda_1,\\dotsc,\\lambda_n)$ con $\\lambda_1\\geq\\lambda_2\\geq\\cdots\\geq\\lambda_D$. Se puede comprobar que $\\boldsymbol{u}_1$ es una dirección de proyección óptima para maximizar la varianza de los datos proyectados, siendo $\\lambda_1$ dicha varianza. Además, entre todas las direcciones ortonormales a $\\boldsymbol{u}_1$, $\\boldsymbol{u}_2$ y $\\lambda_2$ son dirección óptima de proyección y varianza correspondiente. Y así, sucesivamente, hasta $\\boldsymbol{u}_D$ y $\\lambda_D$. PCA es una técnica de **reducción de la dimensión** que proyecta linealmente los datos en el subespacio lineal generado por los $K$ vectores propios asociados a los $K$ mayores valores propios:\n",
    "$$\\operatorname{PCA}(\\mathbf{X})%\n",
    "=(\\mathbf{X}-\\mathbf{1}_N\\boldsymbol{\\mu}^t)\\mathbf{U}_K%\n",
    "\\quad\\text{con}\\quad%\n",
    "\\mathbf{U}_K=(\\boldsymbol{u}_1, \\dotsc, \\boldsymbol{u}_K)$$\n",
    "Nótese que el centrado previo de los datos no es estrictamente necesario, aunque por lo general sí se hace. Por otro lado, la calidad de la proyección suele medirse en términos de varianza explicada (retenida):\n",
    "$$q_K=\\frac{1}{\\operatorname{tr}(\\mathbf{\\Sigma})}\\sum_{k=1}^K\\lambda_k%\n",
    "\\quad\\text{con}\\quad%\n",
    "\\operatorname{tr}(\\mathbf{\\Sigma})=\\sum_{d=1}^D\\lambda_d$$\n",
    "Con base en este criterio de calidad, se suele escoger el menor $K$ que garantice un cierto porcentaje mínimo de varianza explicada (p.e. el $70\\%$, $90\\%$ o $95\\%$). En aprendizaje automático es usual encontrarnos con datos de dimensión muy elevada que, afortunadamente, puede reducirse drásticamente a cambio de reducir un poco la varianza total explicada. Por ejemplo, en clasificación de imágenes, muchas variables corresponden a píxeles que varían muy poco o nada ($\\lambda$ prácticamente nulo), por lo que pueden ignorarse sin efectos negativos significativos; al contrario, la reducción de la dimensión suele facilitar el desarrollo de sistemas precisos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:** $\\mathbf{X}^t=\\begin{bmatrix}-1&0&2&3\\\\-1&2&0&3\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos centrados:\n",
      " [[-2. -2.]\n",
      " [-1.  1.]\n",
      " [ 1. -1.]\n",
      " [ 2.  2.]] \n",
      "Componente principal 1:\n",
      " [0.70710678 0.70710678] \n",
      "Datos reducidos:\n",
      " [-2.82842712  0.          0.          2.82842712]\n",
      "Varianza explicada:  4.0 \n",
      "Calidad de la proyección (%):  80.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([ [-1, -1], [0, 2], [2, 0], [3, 3] ])\n",
    "m = np.mean(X, axis=0)\n",
    "S = np.cov(X.T, bias=True)\n",
    "La, U = np.linalg.eigh(S)\n",
    "i = La.argsort()[::-1]; La = La[i]; U = U[:,i]\n",
    "Xr = (X - m) @ U[:,0]\n",
    "print(\"Datos centrados:\\n\", X - m, \"\\nComponente principal 1:\\n\", U[:,0], \"\\nDatos reducidos:\\n\", Xr)\n",
    "q1 = La[0]\n",
    "print(\"Varianza explicada: \", q1, \"\\nCalidad de la proyección (%): \", 100.0 * q1/np.sum(La))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
