{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descomposición en valores singulares (SVD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición\n",
    "\n",
    "La SVD generaliza la EVD a matrices rectangulares, del tipo $\\mathbf{A}\\in\\mathbf{R}^{m\\times n}$:\n",
    "$$\\mathbf{A}=\\mathbf{U}\\mathbf{S}\\mathbf{V}^t%\n",
    "=\\sigma_1\\boldsymbol{u}_1\\boldsymbol{v}_1^t+\\cdots+\\sigma_r\\boldsymbol{u}_r\\boldsymbol{v}_r^t$$\n",
    "* $\\mathbf{U}\\in\\mathbf{R}^{m\\times m}$ es una matriz de columnas ortonormales ($\\mathbf{U}^t\\mathbf{U}=\\mathbf{I}$) que llamamos **vectores singulares izquierdos**\n",
    "* $\\mathbf{S}\\in\\mathbf{R}^{m\\times n}$ incluye $r=\\min(m, n)$ **valores singulares** $\\sigma_i\\geq 0$ en su diagonal; ceros en el resto\n",
    "* $\\mathbf{V}\\in\\mathbf{R}^{n\\times n}$ es una matriz de filas y columnas ortonormales ($\\mathbf{V}^t\\mathbf{V}=\\mathbf{V}\\mathbf{V}^t=\\mathbf{I}$); sus columnas son los **vectores singulares derechos**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conexión entre la SVD y la EVD\n",
    "\n",
    "Si $\\mathbf{A}$ es una matriz cuadrada, real, simétrica y definida positiva, se cumple que:\n",
    "* Los valores singulares son los valores propios\n",
    "* Los vectores singulares izquierdos y derechos son los vectores propios (salvo cambios de signo):\n",
    "$$\\;\\mathbf{A}=\\mathbf{U}\\mathbf{S}\\mathbf{V}^t=\\mathbf{U}\\mathbf{S}\\mathbf{U}^t=\\mathbf{U}\\mathbf{S}\\mathbf{U}^{-1}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:** $\\;\\mathbf{X}^t=\\begin{bmatrix}-1&0&2&3\\\\-1&2&0&3\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVD:\n",
      " [4. 1.] \n",
      " [[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]] \n",
      "\n",
      "SVD:\n",
      " [[-0.70710678 -0.70710678]\n",
      " [-0.70710678  0.70710678]] \n",
      " [4. 1.] \n",
      " [[-0.70710678 -0.70710678]\n",
      " [-0.70710678  0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([ [-1, -1], [0, 2], [2, 0], [3, 3] ])\n",
    "A = np.cov(X.T, bias=True)\n",
    "L, E = np.linalg.eigh(A)\n",
    "i = L.argsort()[::-1]; L = L[i]; E = E[:,i]\n",
    "print(\"EVD:\\n\", L, \"\\n\", E, \"\\n\")\n",
    "U, S, Vt = np.linalg.svd(A)\n",
    "print(\"SVD:\\n\", U, \"\\n\", S, \"\\n\", Vt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD truncada\n",
    "\n",
    "Sea $\\mathbf{A}=\\mathbf{U}\\mathbf{S}\\mathbf{V}^t$ la SVD de una matriz $\\mathbf{A}\\in\\mathbf{R}^{m\\times n}$. Sea\n",
    "$\\mathbf{\\hat{A}}_K=\\mathbf{U}_K\\mathbf{S}_K\\mathbf{V}_K^t$ una aproximación de $\\mathbf{A}$ de rango $K$ construida a partir de sus $K$ mayores valores singulares, junto con sus $K$ vectores singulares asociados, izquierdos y derechos.\n",
    "Se puede comprobar que $\\mathbf{\\hat{A}}_K$ es la mejor aproximación de $\\mathbf{A}$ en norma Frobenius (al cuadrado),\n",
    "$\\lVert\\mathbf{A}-\\mathbf{\\hat{A}}_K\\rVert_F^2$.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA con la SVD\n",
    "\n",
    "Sea $\\mathbf{X}\\in\\mathbb{R}^{N\\times D}$ una matriz de datos **centrada** y sea $\\mathbf{\\Sigma}=\\frac{1}{N}\\mathbf{X}^t\\mathbf{X}$ su matriz de covarianzas empírica. Podemos reducir la dimensión de $\\mathbf{X}$ a $K$ dimensiones mediante PCA, a partir de la descomposición propia de $\\mathbf{\\Sigma}$ truncada con los $K$ mayores valores propios, $\\mathbf{E}_K\\boldsymbol{\\Lambda}_K\\mathbf{E}_K^t$:\n",
    "$$\\operatorname{PCA}(\\mathbf{X})=\\mathbf{X}\\mathbf{E}_K%\n",
    "\\quad\\text{con}\\quad%\n",
    "\\mathbf{E}_K=(\\boldsymbol{e}_1, \\dotsc, \\boldsymbol{e}_K)$$\n",
    "Nótese que se ha omitido el centrado de datos ya que $\\mathbf{X}$ se asume centrada.\n",
    "\n",
    "Considérese la SVD $K$-truncada de $\\mathbf{X}$, $\\mathbf{\\hat{X}}_K=\\mathbf{U}_K\\mathbf{S}_K\\mathbf{V}_K^t$. Se puede comprobar que vectores singulares derechos de $\\mathbf{X}$ son los vectores propios de $\\mathbf{X}^t\\mathbf{X}$ (y $\\mathbf{\\Sigma}$), por lo que $\\mathbf{V}_K=\\mathbf{E}_K$. Además, los valores propios de $\\mathbf{\\Sigma}$ pueden hallarse a partir de los valores singulares de $\\mathbf{X}$ como $\\lambda_k=\\sigma_k^2/N$. En definitiva, podemos calcular PCA con la SVD:\n",
    "$$\\operatorname{PCA}(\\mathbf{X})=\\mathbf{X}\\mathbf{V}_K%\n",
    "\\approx\\mathbf{U}_K\\mathbf{S}_K\\mathbf{V}_K^t\\mathbf{V}_K%\n",
    "=\\mathbf{U}_K\\mathbf{S}_K$$\n",
    "En general, por motivos computacionales, el cálculo de PCA se suele hacer con la SVD en lugar de la EVD."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:** $\\;\\mathbf{X}^t=\\begin{bmatrix}-1&0&2&3\\\\-1&2&0&3\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos centrados:\n",
      "[[-2. -2.]\n",
      " [-1.  1.]\n",
      " [ 1. -1.]\n",
      " [ 2.  2.]]\n",
      "\n",
      "PCA con la EVD:\n",
      "\n",
      "e1:  [0.70710678 0.70710678] \n",
      "PCA: [-2.82842712  0.          0.          2.82842712]\n",
      "\n",
      "PCA con la SVD:\n",
      "\n",
      "v1:  [0.70710678 0.70710678] \n",
      "PCA: [-2.82842712e+00  2.22044605e-16 -2.22044605e-16  2.82842712e+00]\n",
      "\n",
      "u1:  [-7.07106781e-01  1.07843469e-16 -8.54210252e-18  7.07106781e-01] s1:  4.000000000000001 \n",
      "PCA: [-2.82842712e+00  4.31373875e-16 -3.41684101e-17  2.82842712e+00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([ [-1, -1], [0, 2], [2, 0], [3, 3] ])\n",
    "\n",
    "print(\"Datos centrados:\")\n",
    "m = np.mean(X, axis=0)\n",
    "X = X - m\n",
    "print(X)\n",
    "\n",
    "print(\"\\nPCA con la EVD:\\n\")\n",
    "A = np.cov(X.T, bias=True)\n",
    "L, E = np.linalg.eigh(A)\n",
    "i = L.argsort()[::-1]; L = L[i]; E = E[:,i]\n",
    "e1 = E[:, 0]\n",
    "print(\"e1: \", e1, \"\\nPCA:\", X @ e1)\n",
    "\n",
    "print(\"\\nPCA con la SVD:\\n\", )\n",
    "U, S, Vt = np.linalg.svd(X)\n",
    "v1 = Vt[:, 0]\n",
    "print(\"v1: \", v1, \"\\nPCA:\", X @ v1)\n",
    "u1 = U[:, 0]\n",
    "s1 = S[0]\n",
    "print(\"\\nu1: \", u1, \"s1: \", s1, \"\\nPCA:\", u1 * s1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
