{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificadores naive Bayes: ajuste del modelo\n",
    "\n",
    "## Parámetros\n",
    "\n",
    "El vector de parámetros de un clasificador naive Bayes es $\\boldsymbol{\\theta}=(\\boldsymbol{\\pi}, \\boldsymbol{\\theta}')$, donde $\\boldsymbol{\\pi}$ son las priors de las clases y $\\boldsymbol{\\theta}'=\\{\\boldsymbol{\\theta}_{dc}\\}$ los parámetros que caracterizan la distribuciones característica-clase. La función de probabilidad que caracteriza la distribución a priori de las etiquetas de clase es una categórica gobernada por $\\boldsymbol{\\pi}$:\n",
    "$$p(y\\mid\\boldsymbol{\\pi})%\n",
    "=\\operatorname{Cat}(y\\mid\\boldsymbol{\\pi})%\n",
    "=\\prod_c \\pi_c^{\\mathbb{I}(y=c)}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de verosimilitud conjunta\n",
    "\n",
    "La función de verosimilitud conjunta de los parámetros $\\boldsymbol{\\theta}$ respecto a $N$ datos, $\\mathcal{D}=\\{(\\boldsymbol{x}_n,y_n)\\}$, es:\n",
    "$$\\begin{align*}\n",
    "p(\\mathcal{D}\\mid\\boldsymbol{\\theta})%\n",
    "&=\\prod_n p(\\boldsymbol{x}_n,y_n\\mid\\boldsymbol{\\theta})&&\\text{(muestras i.i.d.)}\\\\%\n",
    "&=\\prod_n p(y_n\\mid\\boldsymbol{\\pi})\\,p(\\boldsymbol{x}_n\\mid y_n, \\boldsymbol{\\theta}')&&\\text{(regla producto)}\\\\%\n",
    "&=\\prod_n \\prod_c (p(y_n=c\\mid\\boldsymbol{\\pi})\\,p(\\boldsymbol{x}_n\\mid y_n=c, \\boldsymbol{\\theta}'))^{\\mathbb{I}(y_n=c)}%\n",
    "&&\\text{(explicitando $c$)}\\\\%\n",
    "&=\\prod_n \\prod_c \\pi_c^{\\mathbb{I}(y_n=c)}\\prod_c p(\\boldsymbol{x}_n\\mid y_n=c, \\boldsymbol{\\theta}')^{\\mathbb{I}(y_n=c)}%\n",
    "&&\\text{(reodenando factores)}\\\\%\n",
    "&=\\prod_n \\prod_c \\pi_c^{\\mathbb{I}(y_n=c)}\\prod_c\\prod_d p(x_{nd}\\mid y_n=c, \\boldsymbol{\\theta}_{dc})^{\\mathbb{I}(y_n=c)}%\n",
    "&&\\text{(asunción NB)}%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de log-verosimilitud conjunta\n",
    "\n",
    "Tomando el logaritmo de la verosimilitud conjunta tenemos:\n",
    "$$\\begin{align*}\n",
    "\\log p(\\mathcal{D}\\mid\\boldsymbol{\\theta})%\n",
    "&=\\left[\\sum_n\\sum_c\\mathbb{I}(y_n=c)\\log\\pi_c\\right] +\n",
    "\\left[\\sum_n\\sum_c\\sum_d\\mathbb{I}(y_n=c)\\log p(x_{nd}\\mid y_n=c, \\boldsymbol{\\theta}_{dc})\\right]\\\\%\n",
    "&=\\left[\\sum_c\\log\\pi_c\\sum_n\\mathbb{I}(y_n=c)\\right] +\n",
    "\\left[\\sum_c\\sum_d\\sum_n\\mathbb{I}(y_n=c)\\log p(x_{nd}\\mid y_n=c, \\boldsymbol{\\theta}_{dc})\\right]\\\\%\n",
    "&=\\sum_c N_c\\log\\pi_c + \\sum_c\\sum_d\\sum_{n:y_n=c}\\log p(x_{nd}\\mid y_n=c, \\boldsymbol{\\theta}_{dc})\\\\%\n",
    "&=\\log p(\\mathcal{D}_y\\mid\\boldsymbol{\\pi}) + \\sum_c\\sum_d\\log p(\\mathcal{D}_{dc}\\mid\\boldsymbol{\\theta}_{dc})%\n",
    "\\end{align*}$$\n",
    "donde $\\mathcal{D}_y=\\{y_n\\}_{n=1}^N$ son las etiquetas y $\\mathcal{D}_{dc}=\\{x_{nd}:y_n=c\\}_{n=1}^N$ son los valores de la característica $d$ en la clase $c$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximización: planteamiento general\n",
    "\n",
    "El MLE de la log-verosimilitud conjunta, $\\hat{\\boldsymbol{\\theta}}$, puede hallarse mediante maximización separada en $\\boldsymbol{\\pi}$ y cada $\\boldsymbol{\\theta}_{dc}$\n",
    "$$\\boldsymbol{\\nabla}_{\\boldsymbol{\\theta}}\\log p(\\mathcal{D}\\mid\\boldsymbol{\\theta})\\rvert_{\\hat{\\boldsymbol{\\theta}}}=\\boldsymbol{0}%\n",
    "\\quad\\Leftrightarrow\\quad%\n",
    "\\boldsymbol{\\nabla}_{\\boldsymbol{\\pi}}\\log p(\\mathcal{D}_y\\mid\\boldsymbol{\\pi})\\rvert_{\\hat{\\boldsymbol{\\pi}}}=\\boldsymbol{0}%\n",
    "\\quad\\text{y}\\quad%\n",
    "\\boldsymbol{\\nabla}_{\\boldsymbol{\\theta}_{dc}}\\log p(\\mathcal{D}_{dc}\\mid\\boldsymbol{\\theta}_{dc})\\rvert_{\\hat{\\boldsymbol{\\theta}}_{dc}}=\\boldsymbol{0}%\n",
    "\\quad\\text{para todo $d$ y $c$}$$\n",
    "La maximización en $\\boldsymbol{\\pi}$ está sujeta a que la suma de priors sea $1$, por lo que introducimos un multiplicador de Lagrange $\\lambda$ para construir una Lagrangiana fácil de optimizar sin restricciones:\n",
    "$$\\hat{\\boldsymbol{\\pi}}%\n",
    "=\\operatorname*{argmax}_{\\boldsymbol{\\pi}}\\;\\max_{\\lambda}\\;L(\\boldsymbol{\\pi}, \\lambda)%\n",
    "\\quad\\text{con}\\quad%\n",
    "L(\\boldsymbol{\\pi}, \\lambda)=\\log p(\\mathcal{D}_y\\mid\\boldsymbol{\\pi})+\\lambda\\left(\\left(\\sum_c\\pi_c\\right)-1\\right)$$\n",
    "Derivando respecto a cada $\\pi_c$ y $\\lambda$, e igualando a cero, obtenemos $\\hat{\\boldsymbol{\\pi}}$:\n",
    "$$\\hat{\\pi}_c=\\frac{N_c}{N}\\quad\\text{con}\\quad N_c=\\sum_{y_n\\in\\mathcal{D}_y}\\mathbb{I}(y_n=c)$$\n",
    "Nótese que este resultado es aplicable con independencia de las distribuciones característica-clase que se escojan. Ahora bien, obviamente, el MLE de cada $\\boldsymbol{\\theta}_{dc}$ depende la distribución escogida."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximización: Bernoulli\n",
    "Si las características son binarias, $x_{nd}\\in\\{0,1\\}$, es fácil comprobar que:\n",
    "$$\\hat{\\theta}_{dc}=\\frac{N_{dc}}{N_c}%\n",
    "\\quad\\text{con}\\quad%\n",
    "N_{dc}=\\sum_{x_{nd}\\in\\mathcal{D}_{dc}}\\mathbb{I}(x_{nd}=1)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:** $\\;C=2$, $\\;p(y=1)=p(y=2)=0.5$, $\\;D=2$, \n",
    "$\\;\\boldsymbol{\\theta}=[\\boldsymbol{\\theta}_1;\\boldsymbol{\\theta}_2]$, $\\;\\boldsymbol{\\theta}_1=(0.7, 0.3)^t$, $\\;\\boldsymbol{\\theta}_2=(0.2, 0.8)^t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta1:  [0.75925926 0.2962963 ]  theta2:  [0.19565217 0.67391304]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multinomial, bernoulli\n",
    "\n",
    "N = 100 # >=2 para tener al menos un dato por clase\n",
    "yy = multinomial(1, [0.5, 0.5]).rvs(N - 2)\n",
    "N1 = yy[yy[:, 0] == 1].shape[0] + 1\n",
    "xxy1 = np.hstack((np.vstack(bernoulli(0.7).rvs(N1)), np.vstack(bernoulli(0.3).rvs(N1))))\n",
    "theta1 = xxy1.mean(axis=0)\n",
    "N2 = N - N1\n",
    "xxy2 = np.hstack((np.vstack(bernoulli(0.2).rvs(N2)), np.vstack(bernoulli(0.8).rvs(N2))))\n",
    "theta2 = xxy2.mean(axis=0)\n",
    "print(\"theta1: \", theta1, \" theta2: \", theta2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximización: categórica\n",
    "Si las características son categóricas, $\\;x_{nd}\\in\\{1,\\dotsc,K\\}$, es fácil comprobar que:\n",
    "$$\\hat{\\theta}_{dck}=\\frac{N_{dck}}{N_c}%\n",
    "\\quad\\text{con}\\quad%\n",
    "N_{dck}=\\sum_{x_{nd}\\in\\mathcal{D}_{dc}}\\mathbb{I}(x_{nd}=k)$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:** $\\;C=2$, $\\;p(y=1)=p(y=2)=0.5$, $\\;D=2$, $\\;K_1=K_2=3$\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{\\theta}_{11}&=(0.6, 0.2, 0.2)^t & \\boldsymbol{\\theta}_{12}&=(0.3, 0.4, 0.3)^t\\\\\n",
    "\\boldsymbol{\\theta}_{21}&=(0.1, 0.3, 0.6)^t & \\boldsymbol{\\theta}_{22}&=(0.3, 0.2, 0.5)^t%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta11:  [0.63333333 0.25       0.11666667]  theta12:  [0.3  0.35 0.35]\n",
      "theta21:  [0.08333333 0.35       0.56666667]  theta22:  [0.25  0.225 0.525]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multinomial\n",
    "\n",
    "N = 100 # >=2 para tener al menos un dato por clase\n",
    "yy = multinomial(1, [0.5, 0.5]).rvs(N - 2)\n",
    "N1 = yy[yy[:, 0] == 1].shape[0] + 1\n",
    "N2 = N - N1\n",
    "theta11 = np.vstack(multinomial(1, [0.6, 0.2, 0.2]).rvs(N1)).mean(axis=0)\n",
    "theta21 = np.vstack(multinomial(1, [0.1, 0.3, 0.6]).rvs(N1)).mean(axis=0)\n",
    "theta12 = np.vstack(multinomial(1, [0.3, 0.4, 0.3]).rvs(N2)).mean(axis=0)\n",
    "theta22 = np.vstack(multinomial(1, [0.3, 0.2, 0.5]).rvs(N2)).mean(axis=0)\n",
    "print(\"theta11: \", theta11, \" theta12: \", theta12)\n",
    "print(\"theta21: \", theta21, \" theta22: \", theta22)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximización: Gaussiana\n",
    "Si las características son reales que siguen una distribución normal, $\\;x_{nd}\\in\\mathbb{R}$, es fácil comprobar que:\n",
    "$$\\begin{align*}\n",
    "\\hat{\\mu}_{dc}&=\\frac{1}{N_c}\\sum_{x_{nd}\\in\\mathcal{D}_{dc}}x_{nd}\\\\%\n",
    "\\hat{\\sigma}_{dc}^2&=\\frac{1}{N_c}\\sum_{x_{nd}\\in\\mathcal{D}_{dc}}(x_{nd}-\\hat{\\mu}_{dc})^2%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:** $\\;C=2$, $\\;p(y=1)=p(y=2)=0.5$, $\\;D=2$,\n",
    "$\\;\\boldsymbol{\\theta}=[\\boldsymbol{\\theta}_1;\\boldsymbol{\\theta}_2]$\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{\\theta}_1&=(\\boldsymbol{\\mu}_1,\\mathbf{\\Sigma}_1)%\n",
    "&\\boldsymbol{\\mu}_1&=(\\mu_{11},\\mu_{21})^t=(-2,0)^t%\n",
    "&\\mathbf{\\Sigma}_1&=\\operatorname{diag}(\\sigma_{11}^2, \\sigma_{21}^2)=\\mathbf{I}_2\\\\%\n",
    "\\boldsymbol{\\theta}_2&=(\\boldsymbol{\\mu}_2,\\mathbf{\\Sigma}_2)%\n",
    "&\\boldsymbol{\\mu}_2&=(\\mu_{12},\\mu_{22})^t=(2,0)^t%\n",
    "&\\mathbf{\\Sigma}_2&=\\operatorname{diag}(\\sigma_{12}^2, \\sigma_{22}^2)=\\mathbf{I}_2%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu1:  [-1.90842173 -0.08541221]  Sigma1:  [0.98908653 0.80654357]\n",
      "mu2:  [2.18078546 0.01006511]  Sigma2:  [0.94451474 1.04591484]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multinomial, multivariate_normal\n",
    "\n",
    "N = 100 # >=2 para tener al menos un dato por clase\n",
    "yy = multinomial(1, [0.5, 0.5]).rvs(N - 2)\n",
    "N1 = yy[yy[:, 0] == 1].shape[0] + 1\n",
    "N2 = N - N1\n",
    "xxy1 = multivariate_normal([-2, 0], np.eye(2)).rvs(N1)\n",
    "mu1 = xxy1.mean(axis=0)\n",
    "Sigma1 = np.var(xxy1, axis=0)\n",
    "xxy2 = multivariate_normal([2, 0], np.eye(2)).rvs(N2)\n",
    "mu2 = xxy2.mean(axis=0)\n",
    "Sigma2 = np.var(xxy2, axis=0)\n",
    "print(\"mu1: \", mu1, \" Sigma1: \", Sigma1)\n",
    "print(\"mu2: \", mu2, \" Sigma2: \", Sigma2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
